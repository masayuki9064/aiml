{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# porto-seguro-safe-driver-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "import time\n",
    "import shutil\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import csv\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "from catboost import Pool,CatBoostClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%precision %08.2f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大表示列数の指定（ここでは2000列を指定）\n",
    "pd.set_option('display.max_columns', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大表示行数の指定（ここでは200行を指定）\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators:ここを増やすと精度は上がるが遅い。。。\n",
    "ESTIMATOR_NUM=10\n",
    "RANDOM_STATE_VAL = 0\n",
    "target_col_name = \"target\"\n",
    "id_col_name = \"id\"\n",
    "CV_VAL = 5\n",
    "\n",
    "TEST_SIZE_VAL=0.2\n",
    "#FEATURE_NUM = 950\n",
    "#FEATURE_NUM = 150\n",
    "FEATURE_NUM = 200\n",
    "\n",
    "SELECTOR_STEP = .05\n",
    "\n",
    "\n",
    "PROBA_THRESHOLD=0.7\n",
    "#PROBA_THRESHOLD=0.6\n",
    "#PROBA_THRESHOLD=0.5\n",
    "\n",
    "# カテゴリ変数に変換する対象のカラム名\n",
    "CONVERT_TO_CATEGORY_COL_NAME = [\"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_13\" ,\"ps_car_14\", \"ps_car_15\"]\n",
    "\n",
    "\n",
    "# 特徴量-targetの相関の絶対値が小さいので消すしきい値\n",
    "#DEL_FEATURE_TARGET_CORR_COL_THRESHOLD = 0.001\n",
    "DEL_FEATURE_TARGET_CORR_COL_THRESHOLD = 0.005\n",
    "#DEL_FEATURE_TARGET_CORR_COL_THRESHOLD = 0.01\n",
    "\n",
    "\n",
    "# 特徴量同士の相関の絶対値が大きいので消すしきい値\n",
    "DEL_BIG_CORR_COL_THRESHOLD = 0.8\n",
    "\n",
    "# # 歪度の絶対値が大きいので消すカラム数\n",
    "# DEL_SKEW_COL_NUM = 3\n",
    "# 歪度の絶対値が大きいので消す対象のカラムのしきい値\n",
    "DEL_SKEW_THRESHOLD = 20.0\n",
    "\n",
    "# 尖度の絶対値が大きいので消す対象のカラムのしきい値\n",
    "#DEL_KURT_THRESHOLD = 100.0\n",
    "#DEL_KURT_THRESHOLD = 150.0\n",
    "DEL_KURT_THRESHOLD = 500.0\n",
    "\n",
    "\n",
    "# 不正値をnp.NaNに置換するカラム名のリスト\n",
    "#ILLEGAL_REPLACE_NAN_COL_NAME_LIST = [\"ps_ind_05_cat\", \"ps_car_01_cat\", \"ps_reg_03\", \"ps_car_12\", \"ps_car_14\"]\n",
    "ILLEGAL_REPLACE_NAN_COL_NAME_LIST = [\"ps_ind_02_cat\", \"ps_ind_04_cat\", \"ps_ind_05_cat\",\n",
    "                                     \"ps_reg_03\",\n",
    "                                     \"ps_car_01_cat\", \"ps_car_02_cat\", \"ps_car_03_cat\", \"ps_car_05_cat\", \"ps_car_07_cat\", \"ps_car_09_cat\", \n",
    "                                     \"ps_car_11\", \"ps_car_12\", \"ps_car_14\"]\n",
    "\n",
    "\n",
    "# 外れ値をnp.NaNに置換するカラム名のリスト\n",
    "#OUTLIER_REPLACE_NAN_COL_NAME_LIST = [\"ps_reg_03\", \"ps_car_12\", \"ps_car_14\"]\n",
    "#OUTLIER_REPLACE_NAN_COL_NAME_LIST = [\"ps_reg_03\", \"ps_car_13\"]\n",
    "OUTLIER_REPLACE_NAN_COL_NAME_LIST = [\"ps_car_13\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispalyDf(readDf, dispFlag, row_num, dispLabel):\n",
    "    print(\"========================= {} =========================\".format(dispLabel))\n",
    "    row_num , col_num = np.shape(readDf)\n",
    "    print(\"row_num = {}, col_num = {}\".format(row_num, col_num))\n",
    "    if dispFlag:\n",
    "        display(readDf.head(row_num))\n",
    "    print(\"++++++ None Count All++++++\")\n",
    "    print(readDf.isnull().values.sum())\n",
    "    print(\"++++++ None Count ++++++\")\n",
    "    print(readDf.isnull().sum())\n",
    "    print(\"++++++ Col Type ++++++\")\n",
    "    print(readDf.dtypes)\n",
    "    print(\"++++++ Unique By Col（Noneは含まないので注意） ++++++\")\n",
    "    for col_name in readDf.columns.tolist():\n",
    "        print(\"{} : {}\".format(col_name, readDf[col_name].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "######## チェック関数\n",
    "################################################\n",
    "def chkVal(expect_label, expect_num, act_label, act_num):\n",
    "    if expect_num != act_num:\n",
    "        raise Exception(\"Num Error !! expect({}): {}, act({}) : {}\"\n",
    "                        .format(expect_label, expect_num, act_label, act_num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getStandardScalerDf(inputDf):\n",
    "    row_num, col_num = np.shape(inputDf)\n",
    "    \n",
    "    # スケール変換\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    val_array = scaler.fit_transform(np.array(inputDf.values.tolist()))\n",
    "    standardScDf = pd.DataFrame(val_array, dtype=np.float, columns = inputDf.columns.tolist())\n",
    "    \n",
    "    sc_row_num, sc_col_num = np.shape(standardScDf)\n",
    "\n",
    "    # 行数チェック\n",
    "    chkVal(\"expect_row_num\", row_num, \"sc_row_num\", sc_row_num)\n",
    "    # 列数チェック\n",
    "    chkVal(\"expect_col_num\", col_num, \"sc_col_num\", sc_col_num)\n",
    "\n",
    "    \n",
    "    del val_array\n",
    "    return standardScDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test用のカラムのDataFrame\n",
    "def getAdjustTestColDf(inputDf, train_col_list):\n",
    "    not_enough_col_list = set(train_col_list) - set(inputDf.columns.tolist())\n",
    "    print(\"# trainに有ってtestにないカラム名のリスト\")\n",
    "    print(\"{} : {}\".format(len(not_enough_col_list), not_enough_col_list))\n",
    "        \n",
    "    dataLen = len(inputDf)\n",
    "        \n",
    "    retDf = inputDf.join(\n",
    "        pd.DataFrame({col_name : [0]*dataLen  for col_name  in not_enough_col_list },\n",
    "                     dtype=np.float, columns=not_enough_col_list)\n",
    "    ).loc[:, train_col_list]\n",
    "    \n",
    "    # カラムリストが一致するかチェック\n",
    "    if retDf.columns.tolist() != train_col_list:\n",
    "        raise Exception(\"col_name_list not Match train and test.\")\n",
    "\n",
    "    # データ行数が一致するかチェック\n",
    "    chkVal(\"dataLen\", dataLen, \"retDf\", len(retDf))\n",
    "\n",
    "    return retDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(RANDOM_STATE_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_read_Df = pd.read_csv('input/train.csv',dtype={id_col_name:object})\n",
    "act_test_read_Df = pd.read_csv('input/test.csv',dtype={id_col_name:object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df = act_train_read_Df.copy()\n",
    "act_test_Df = act_test_read_Df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dispalyDf(act_train_Df, True, 10, \"act_train_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(act_test_Df, True, 10, \"act_test_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train:データのDataFrameと、ターゲットのデータフレームに分ける。\n",
    "train_target_Ser = act_train_Df[target_col_name]\n",
    "act_train_Df = act_train_Df.loc[:,[col_name for col_name in act_train_Df.columns.tolist() if col_name not in [id_col_name,target_col_name]]]\n",
    "print(set(train_target_Ser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_Ser.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:idとデータのDataFrameを抽出。\n",
    "test_id_Df = act_test_Df.loc[:,[id_col_name]]\n",
    "act_test_Df = act_test_Df.loc[:,[col_name for col_name in act_test_Df.columns.tolist() if col_name != id_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"trainのカラム数 = \",len(act_train_Df.columns.tolist()))\n",
    "print(\"testのカラム数 = \",len(act_test_Df.columns.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼▼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtDf = act_train_Df.kurt().reset_index()\n",
    "kurtDf.columns=[\"col_name\",\"kurt\"]\n",
    "#skews_col_list = skewDf[skewDf[\"skew\"]>DEL_SKEW_THRESHOLD][\"col_name\"].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtDf[kurtDf[\"kurt\"].abs()>10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minSer = act_train_Df.loc[:,].min()\n",
    "print(minSer[minSer==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDf = act_train_Df.corr().abs().min().reset_index()\n",
    "tempDf.columns=[\"index\",\"val\"]\n",
    "tempDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDf.sort_values(\"val\").tail(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df.corr().abs().min()[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "feat_corr = set()\n",
    "corr_matrix = act_train_Df.corr()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            feat_name = corr_matrix.columns[i]\n",
    "            feat_corr.add(feat_name)\n",
    "\n",
    "print(len(set(feat_corr)))\n",
    "# -> 658"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(feat_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_reg_01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDf = act_train_Df.corr().abs().min().reset_index()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempDf.columns=[\"col_name\",\"corr\"]\n",
    "tempDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_ind_12_bin\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_ind_12_bin\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_ind_14\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_ind_14\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_car_10_cat\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_car_10_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df[\"ps_car_02_cat\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name_list = []\n",
    "for col_name in act_train_Df.columns.tolist():\n",
    "    if  \"_cat\"  in col_name or \"_bin\"  in col_name:\n",
    "        if len([val for val in act_train_Df[col_name].values.tolist() if val <0]) > 0:\n",
    "            print(\"========== {} ==========\".format(col_name))\n",
    "            print(act_train_Df[col_name].value_counts())\n",
    "            col_name_list.append(col_name)\n",
    "\n",
    "            \n",
    "print(col_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフのサイズを指定\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "# 箱ひげ図を表示\n",
    "plt.subplot(1, 4, 1)\n",
    "sns.boxplot(data=act_train_Df[\"ps_reg_03\"])\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "sns.boxplot(data=act_train_Df[\"ps_car_12\"])\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "sns.boxplot(data=act_train_Df[\"ps_car_13\"])\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "sns.boxplot(data=act_train_Df[\"ps_car_14\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフのサイズを指定\n",
    "plt.figure(figsize=(20, 4))\n",
    "\n",
    "# ヒストグラムを表示\n",
    "plt.subplot(1, 4, 1)\n",
    "plt.hist(act_train_Df[\"ps_reg_03\"], bins=4)\n",
    "\n",
    "plt.subplot(1, 4, 2)\n",
    "plt.hist(act_train_Df[\"ps_car_12\"], bins=20)\n",
    "\n",
    "plt.subplot(1, 4, 3)\n",
    "plt.hist(act_train_Df[\"ps_car_13\"], bins=20)\n",
    "\n",
    "plt.subplot(1, 4, 4)\n",
    "plt.hist(act_train_Df[\"ps_car_14\"], bins=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df_cp = act_train_Df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmpSer=act_train_Df_cp[\"ps_ind_05_cat\"].replace(-1, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list = act_train_Df_cp[tmpSer.notnull()][\"ps_ind_05_cat\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmpSer=np.where(tmpSer.isnull(), np.random.choice(unique_list, 1)[0], tmpSer)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "list(set(tmpSer.tolist()))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmpSer.unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tmpSer.fillna(tmpSer.mean()).unique()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    targetDf = pd.DataFrame(target_Ser, dtype=int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDf = None\n",
    "for col_name in act_train_Df.columns.tolist():\n",
    "    tempDf = act_train_Df.loc[:,[col_name]].join(pd.DataFrame(train_target_Ser, dtype=int)).corr().abs().loc[[col_name],[train_target_Ser.name]]\n",
    "    if totalDf is None:\n",
    "        totalDf = tempDf.copy()\n",
    "    else:\n",
    "        totalDf = pd.concat([totalDf, tempDf])\n",
    "    del tempDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDf = totalDf.sort_values(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDf.head(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalDf[totalDf[train_target_Ser.name]<DEL_FEATURE_TARGET_CORR_COL_THRESHOLD].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲▲"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentile_Arr(val_Arr):\n",
    "    val_Arr = val_Arr*100\n",
    "    p25_val = np.percentile(val_Arr,25)\n",
    "    p50_val = np.percentile(val_Arr,50)\n",
    "    p75_val = np.percentile(val_Arr,75)\n",
    "\n",
    "    # 低い順に評価していく。\n",
    "    # 25%未満\n",
    "    val_Arr = np.where(val_Arr< p25_val,1,val_Arr)\n",
    "    #25%以上50%未満\n",
    "    val_Arr = np.where(((val_Arr>= p25_val) & ((val_Arr<p50_val))) ,2,val_Arr)\n",
    "    #50%以上75%未満\n",
    "    val_Arr = np.where(((val_Arr>= p50_val) & ((val_Arr<p75_val))) ,3,val_Arr)\n",
    "    #75%以上\n",
    "    val_Arr = np.where(val_Arr>= p75_val ,4,val_Arr)\n",
    "    \n",
    "    return val_Arr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getDropListEqualVal(input_Df, drop_columns):\n",
    "    col_list_1st = input_Df.columns.tolist()\n",
    "    col_list_2nd = col_list_1st.copy()\n",
    "    \n",
    "    for col_name1 in col_list_1st:\n",
    "        col_list_2nd.remove(col_name1)\n",
    "        \n",
    "        drop_columns.extend([col_name2 for col_name2 in col_list_2nd\n",
    "            if col_name2 not in drop_columns and (input_Df[col_name1] == input_Df[col_name2]).all()\n",
    "        ])\n",
    "    \n",
    "    return drop_columns"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def getSerTestValInRange(inputSer, val_range_list):\n",
    "    train_min_val = val_range_list[0]\n",
    "    train_max_val = val_range_list[1]\n",
    "    return np.where(inputSer<train_min_val,train_min_val, \n",
    "                         np.where(inputSer>train_max_val, train_max_val, inputSer))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_del_colmns(input_Df, target_Ser):\n",
    "    drop_columns = []\n",
    "    \n",
    "    # 値のユニーク数が１のカラムをdropリストに追加する\n",
    "    drop_columns.extend([col_name for col_name in input_Df.columns.tolist()\n",
    "                        if input_Df[col_name].nunique() == 1 not in drop_columns])\n",
    "\n",
    "#     # 値が同じカラムをdropリストに追加する。\n",
    "#     drop_columns = getDropListEqualVal(input_Df, drop_columns)\n",
    "\n",
    "    # 相関が大きい特徴量の片方を消す\n",
    "    feat_corr = set()\n",
    "    corr_matrix = input_Df.corr()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > DEL_BIG_CORR_COL_THRESHOLD:\n",
    "                feat_name = corr_matrix.columns[i]\n",
    "                feat_corr.add(feat_name)\n",
    "    \n",
    "    corr_col_list = list(set(feat_corr))\n",
    "    print(\"del corr_col_list = \",corr_col_list)\n",
    "    drop_columns.extend(corr_col_list)\n",
    "    \n",
    "    # targetとの相関が小さい特徴量を消す\n",
    "    target_col_name = target_Ser.name\n",
    "    totalDf = None\n",
    "    for col_name in input_Df.columns.tolist():\n",
    "        tempDf = input_Df.loc[:,[col_name]].join(pd.DataFrame(target_Ser, dtype=int)).corr().abs().loc[\n",
    "            [col_name],[target_col_name]]\n",
    "        if totalDf is None:\n",
    "            totalDf = tempDf.copy()\n",
    "        else:\n",
    "            totalDf = pd.concat([totalDf, tempDf])\n",
    "        del tempDf\n",
    "    to_target_smal_corr_drop_col_list = totalDf[totalDf[target_col_name] < DEL_FEATURE_TARGET_CORR_COL_THRESHOLD].index.tolist()\n",
    "    print(\"del to_target_smal_corr_drop_col_list = \",to_target_smal_corr_drop_col_list)\n",
    "    drop_columns.extend(to_target_smal_corr_drop_col_list)\n",
    "    del totalDf\n",
    "    \n",
    "    # 歪度の絶対値が大きいカラムをdropリストに追加する。\n",
    "    skewDf = input_Df.skew().reset_index()\n",
    "    skewDf.columns=[\"col_name\",\"skew\"]\n",
    "    skews_col_list = skewDf[skewDf[\"skew\"].abs()>DEL_SKEW_THRESHOLD][\"col_name\"].values.tolist()\n",
    "    del skewDf\n",
    "    print(\"del skews_col_list = \",skews_col_list)\n",
    "    drop_columns.extend(skews_col_list)\n",
    "\n",
    "    # 尖度の絶対値が大きいカラムをdropリストに追加する。\n",
    "    kurtDf = input_Df.kurt().reset_index()\n",
    "    kurtDf.columns=[\"col_name\",\"kurt\"]\n",
    "    kurts_col_list = kurtDf[kurtDf[\"kurt\"].abs()>DEL_KURT_THRESHOLD][\"col_name\"].values.tolist()\n",
    "    del kurtDf\n",
    "    print(\"del kurts_col_list = \",kurts_col_list)\n",
    "    drop_columns.extend(kurts_col_list)\n",
    "\n",
    "    \n",
    "    # カラム名の重複を無くす。\n",
    "    drop_columns = list(set(drop_columns))\n",
    "    print(\"drop_columns = \",drop_columns)\n",
    "\n",
    "    return drop_columns\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dropna_Df(inputDf, target_Ser):\n",
    "    \n",
    "    input_colums = inputDf.columns.tolist()\n",
    "    \n",
    "    retDf = inputDf.copy()\n",
    "    targetDf = pd.DataFrame(target_Ser, dtype=int)\n",
    "    retDf = retDf.join(targetDf)\n",
    "\n",
    "    for col_name in ILLEGAL_REPLACE_NAN_COL_NAME_LIST:\n",
    "        print(\"================== {} =================\".format(col_name))\n",
    "        print(retDf[col_name].value_counts())\n",
    "        tmpSer=retDf[col_name].replace(-1, np.nan)\n",
    "        retDf[col_name] = tmpSer\n",
    "\n",
    "    # targetが1\n",
    "    # または\n",
    "    # nanを含まない行かつtargetが0\n",
    "    retDf = retDf[(retDf[target_Ser.name] == 1) | (retDf.notnull().all(axis=1)  & (retDf[target_Ser.name] == 0))]\n",
    "\n",
    "    # nanを-1に戻す\n",
    "    retDf=retDf.replace(np.nan, -1)\n",
    "\n",
    "    ##\n",
    "    del inputDf\n",
    "    \n",
    "    return retDf.loc[:,input_colums], retDf[target_Ser.name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fillna_Df(inputDf):\n",
    "    \n",
    "    retDf = inputDf.copy()\n",
    "\n",
    "    \n",
    "#     for col_name in ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat',\n",
    "#                      'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_03_cat', 'ps_car_05_cat', 'ps_car_07_cat', 'ps_car_09_cat']:\n",
    "#     for col_name in ['ps_ind_02_cat', 'ps_ind_04_cat', 'ps_ind_05_cat',\n",
    "#                      'ps_car_01_cat', 'ps_car_02_cat', 'ps_car_07_cat', 'ps_car_09_cat']:\n",
    "#    for col_name in ['ps_ind_05_cat', 'ps_car_01_cat']:\n",
    "    for col_name in ILLEGAL_REPLACE_NAN_COL_NAME_LIST:\n",
    "        print(\"================== {} =================\".format(col_name))\n",
    "        print(retDf[col_name].value_counts())\n",
    "        tmpSer=retDf[col_name].replace(-1, np.nan)\n",
    "\n",
    "        if  col_name.endswith(\"_cat\"):\n",
    "            # NaNをランダムチョイスで埋める。\n",
    "            unique_list = retDf[tmpSer.notnull()][col_name].unique().tolist()\n",
    "            tmpSer=np.where(tmpSer.isnull(), np.random.choice(unique_list, 1)[0], tmpSer)\n",
    "        else:\n",
    "            # NaNを前後の値で埋める。\n",
    "#             tmpSer = tmpSer.fillna(method=\"ffill\")\n",
    "#             tmpSer = tmpSer.fillna(method=\"bfill\")\n",
    "#             # 平均値で埋める\n",
    "#              tmpSer = tmpSer.fillna(tmpSer.mean())\n",
    "            # 中央値で埋める\n",
    "            tmpSer = tmpSer.fillna(tmpSer.median())\n",
    "\n",
    "\n",
    "        retDf[col_name] = tmpSer\n",
    "        print(retDf[col_name].value_counts())\n",
    "    ##\n",
    "    del inputDf\n",
    "    \n",
    "    return retDf"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def get_outler_replace_Df(inputDf):\n",
    "    retDf = inputDf.copy()\n",
    "    \n",
    "    for col_name in OUTLIER_REPLACE_NAN_COL_NAME_LIST:\n",
    "        #四分位数\n",
    "        q1 =  retDf[col_name].describe()['25%']\n",
    "        q3 =  retDf[col_name].describe()['75%']\n",
    "        iqr = q3 - q1 #四分位範囲\n",
    "\n",
    "        #外れ値の基準点\n",
    "        outlier_min = q1 - (iqr) * 1.5\n",
    "        outlier_max = q3 + (iqr) * 1.5\n",
    "\n",
    "        #範囲から外れている値をNaNに変換する。\n",
    "        retDf[col_name]=np.where(retDf[col_name]<outlier_min, np.nan, retDf[col_name])\n",
    "        retDf[col_name]=np.where(retDf[col_name]>outlier_max, np.nan, retDf[col_name])\n",
    "\n",
    "        # NaNを前後の値で埋める。\n",
    "        retDf[col_name] = retDf[col_name].fillna(method=\"ffill\")\n",
    "        retDf[col_name] = retDf[col_name].fillna(method=\"bfill\")\n",
    "    ##\n",
    "    del inputDf\n",
    "    \n",
    "    return retDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 前処理 ######\n",
    "def getBeroreProcessDf(input_Df, drop_columns, trainFlag, train_val_range_dict, target_Ser):\n",
    "    retDf = input_Df.copy()\n",
    "\n",
    "    #### 訓練時のみの処理\n",
    "    if trainFlag:\n",
    "        # 欠損値の行を削除する。\n",
    "        retDf, target_Ser = get_dropna_Df(retDf, target_Ser)\n",
    "    ####\n",
    "    \n",
    "    # 欠損値を補完する。\n",
    "    retDf = get_fillna_Df(retDf)\n",
    "    \n",
    "#     # 外れ値をnanに変換して補完する。\n",
    "#     retDf = get_outler_replace_Df(retDf)\n",
    "    \n",
    "\n",
    "#     # 連続変数をカテゴリ変数に変換する\n",
    "#     for col_name in CONVERT_TO_CATEGORY_COL_NAME:\n",
    "#         retDf[col_name] = percentile_Arr(retDf[col_name].values).astype(np.int32)\n",
    "#         retDf = retDf.rename(columns={col_name: '{}_cat'.format(col_name)})\n",
    "\n",
    "    #### 訓練時のみの処理\n",
    "    if trainFlag:\n",
    "        # 削除対象カラム名の取得\n",
    "        drop_columns = get_del_colmns(input_Df, target_Ser)\n",
    "    \n",
    "    ####\n",
    "    \n",
    "    retDf = retDf.drop(drop_columns, axis=1)\n",
    "    \n",
    "    columns = retDf.columns.tolist()\n",
    "\n",
    "\n",
    "        \n",
    "###########        \n",
    "#     for col_name in columns:\n",
    "#         if retDf[col_name].dtype == object:\n",
    "#             # 文字列のカラムの加工処理\n",
    "#             retDf[col_name] = retDf[col_name].fillna('type 0')\n",
    "#             retDf[col_name] = retDf[col_name].apply(lambda x: x.split(' ')[1])\n",
    "            \n",
    "#         # int64に変換する。\n",
    "#         retDf[col_name] = pd.to_numeric(retDf[col_name])\n",
    "#         elif retDf[col_name].dtype == np.int64:\n",
    "#             # float64に変換する。\n",
    "#             retDf[col_name] = retDf[col_name].astype(np.float64)\n",
    "\n",
    "            \n",
    "#         if trainFlag:\n",
    "#             # train時\n",
    "#             # 最小と最大を取得\n",
    "#             train_val_range_dict[col_name] = [\n",
    "#                 retDf[col_name].min(), retDf[col_name].max()\n",
    "#             ]\n",
    "#         else:\n",
    "#             # test時\n",
    "#             # 最小〜最大の範囲内に収める\n",
    "#             retDf[col_name]  = getSerTestValInRange(retDf[col_name] , train_val_range_dict[col_name])\n",
    "        \n",
    "     \n",
    "#     # 桁数が大きい値をlogにする\n",
    "#     retDf[\"var38\"] = np.log(retDf[\"var38\"])\n",
    "    \n",
    "    return retDf, drop_columns, train_val_range_dict, target_Ser\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df_2, drop_columns, train_val_range_dict, train_target_Ser = getBeroreProcessDf(act_train_Df, None, True, None, train_target_Ser)\n",
    "# デバッグ用\n",
    "#act_train_Df_2, drop_columns, train_val_range_dict = getBeroreProcessDf(act_train_Df[:1000], drop_columns, True, train_val_range_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(act_train_Df_2, True, 10, \"act_train_Df_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target_Ser.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 特長量のカラム名のディクショナリを作る\n",
    "col_name_dict = {\n",
    "    index: col_name\n",
    "    for index, col_name in  enumerate(act_train_Df_2.columns.tolist())\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "col_name_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_test_Df_2, _, _, _ = getBeroreProcessDf(act_test_Df, drop_columns, False, train_val_range_dict, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(act_test_Df_2, True, 10, \"act_test_Df_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "##### 次元削減(特徴量選択) を使用する\n",
    "##### ↓特徴量選択\n",
    "####################################\n",
    "def getSelectorFeature(inputFeatureDf,y_Df,feature_num, selector_step, model):\n",
    "    \n",
    "    return inputFeatureDf\n",
    "\n",
    "# 以降の処理を首絞め\n",
    "#     print(\"特徴量選択数 = \",feature_num)\n",
    "#     if(feature_num < 0):\n",
    "#         # デフォルト値\n",
    "#         print(\"n_features_to_select = None に設定\")\n",
    "#         feature_num = None\n",
    "#     elif feature_num > len(inputFeatureDf.columns.tolist()):\n",
    "#         print (\"feature_num({}) is Over colnum({})\".format(feature_num, len(inputFeatureDf.columns.tolist())))\n",
    "#         # 次元削減せずに返却する\n",
    "#         return inputFeatureDf\n",
    "    \n",
    "#     # 特徴量選択\n",
    "#     # step:特徴量削除の速度。一度の再帰処理により指定ステップ分の特徴量が消滅する。\n",
    "# #     selector = RFE(estimator=LogisticRegression(penalty='l2', C=1.5, random_state=RANDOM_STATE_VAL, max_iter=120,\n",
    "# #                                            solver='liblinear'),\n",
    "#     selector = RFE(estimator=model,\n",
    "#                    n_features_to_select=feature_num,\n",
    "#                    step=selector_step)\n",
    "#     selector.fit(inputFeatureDf,y_Df.values.ravel())\n",
    "#     #selector.fit(inputFeatureDf, y_Df.iloc[:,0])\n",
    "\n",
    "#     # 削除対象配列(Falseが削除対象)\n",
    "#     selFlagArray = selector.support_\n",
    "    \n",
    "#     # 現状のカラム名を取得\n",
    "#     columnIndex = inputFeatureDf.columns\n",
    "    \n",
    "#     # 削除対象カラム名リスト\n",
    "#     drop_column_list = list()\n",
    "#     # 削除対象配列 でループ(i=0,1,2,・・・)\n",
    "#     #print(len(selFlagArray))\n",
    "#     for i in range(len(selFlagArray)):\n",
    "#         # Falseの場合に、削除対象リストに追加\n",
    "#         if(selFlagArray[i] == False):\n",
    "#             drop_column_list.append(columnIndex[i])\n",
    "\n",
    "#     # 不要な列削除\n",
    "#     #print(\"削除対象カラム = \" , drop_column_list)\n",
    "#     inputFeatureNewDf = inputFeatureDf.drop(drop_column_list, axis=1)\n",
    "    \n",
    "\n",
    "#     return inputFeatureNewDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelineに使う独自関数\n",
    "class MyStandardScaler(object):\n",
    "    def fit(self, X, y):\n",
    "        print(\" MyStandardScaler.fit\")\n",
    "        return X\n",
    "    def fit_transform(self, X, y):\n",
    "        print(\" MyStandardScaler.fit_transform\")\n",
    "        return getStandardScalerDf(X)\n",
    "#    def transform(self, X, y):\n",
    "    def transform(self, X):\n",
    "        print(\" MyStandardScaler.transform\")\n",
    "        return getStandardScalerDf(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipelineに使う独自関数\n",
    "class MyPreprocessor(object,):\n",
    "    def fit(self, X, y):\n",
    "        print(\" MyPreprocessor.fit\")\n",
    "        return X\n",
    "    def fit_transform(self, X, y, feature_num, selector_step, model):\n",
    "        print(\" MyPreprocessor.fit_transform\")\n",
    "        return getSelectorFeature(X,y,feature_num, selector_step, model)\n",
    "#        return getSelectorFeature(X)\n",
    "    def predict(self, X, train_col_list):\n",
    "        print(\" MyPreprocessor.predict\")\n",
    "        return getAdjustTestColDf(X, train_col_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データを学習用とバリデーション用に分ける\n",
    "X_train_feature, X_train_validation, y_train_answer, y_train_validation = train_test_split(act_train_Df_2,     # 特徴量\n",
    "                                             train_target_Ser,    # 正解データ\n",
    "                                             test_size=TEST_SIZE_VAL ,\n",
    "                                             random_state=RANDOM_STATE_VAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(X_train_feature, True, 10, \"X_train_feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_traind_df = pd.DataFrame(y_train_validation)\n",
    "print(y_traind_df)\n",
    "print(y_traind_df.dtypes)\n",
    "\n",
    "print(y_traind_df[y_traind_df[\"target\"]==1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_answer.value_counts())\n",
    "print(y_train_validation.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 学習 ####\n",
    "model_def_dict = {\n",
    "    ##### LGBMClassifier #####\n",
    "#    \"LGBMClassifier_Default\":   LGBMClassifier(random_state=RANDOM_STATE_VAL),\n",
    "    \"LGBMClassifier_max_depth_5\":   LGBMClassifier(random_state=RANDOM_STATE_VAL, max_depth=5),\n",
    "\n",
    "# ボツ\n",
    "#    \"LGBMClassifier_max_depth_3\":   LGBMClassifier(random_state=RANDOM_STATE_VAL, max_depth=3),\n",
    "#    \"LGBMClassifier_max_depth_4\":   LGBMClassifier(random_state=RANDOM_STATE_VAL, max_depth=4),\n",
    "#    \"LGBMClassifier_max_depth_6\":   LGBMClassifier(random_state=RANDOM_STATE_VAL, max_depth=6),\n",
    "#    \"LGBMClassifier_learning_rate_0.01\":   LGBMClassifier(random_state=RANDOM_STATE_VAL, learning_rate=0.01),\n",
    "    \n",
    "    \n",
    "\n",
    "    ##### XGBClassifier #####\n",
    "#     \"XGBClassifier_Default\":   XGBClassifier(random_state=RANDOM_STATE_VAL),\n",
    "#     \"XGBClassifier_max_depth_3\":   XGBClassifier(random_state=RANDOM_STATE_VAL, max_depth=3),\n",
    "\n",
    "# ボツ\n",
    "#    \"XGBClassifier_max_depth_4\":   XGBClassifier(random_state=RANDOM_STATE_VAL, max_depth=4),\n",
    "#    \"XGBClassifier_max_depth_5\":   XGBClassifier(random_state=RANDOM_STATE_VAL, max_depth=5),\n",
    "\n",
    "    \n",
    "    ##### CatBoostClassifier #####\n",
    "    # ハイパーパラメータの調整はしすぎるとダメ！！\n",
    "    #early_stopping_roundsは小さすぎると学習不足につながる。むしろ指定しないほうがいい。\n",
    "    \n",
    "#    \"CatBoostClassifier_Default\": CatBoostClassifier(random_seed=RANDOM_STATE_VAL),\n",
    "    \n",
    "#     \"CatBoostClassifier_200\": CatBoostClassifier(random_seed=RANDOM_STATE_VAL,\n",
    "#                                                  eval_metric=\"AUC\",\n",
    "#                                                 learning_rate=0.0156,\n",
    "#                                                  early_stopping_rounds=300,\n",
    "# #                                                 iterations = 20000),\n",
    "#                                                  iterations = 2000),\n",
    "\n",
    "                                                \n",
    "##############    \n",
    "#     ExtraTreesClassifier(n_estimators = ESTIMATOR_NUM),\n",
    "#     RandomForestClassifier(n_estimators=ESTIMATOR_NUM, random_state=RANDOM_STATE_VAL),\n",
    "#     DecisionTreeClassifier(),\n",
    "#     LogisticRegression(penalty='l2', C=1.5, random_state=RANDOM_STATE_VAL,max_iter=120,\n",
    "#                        solver=\"liblinear\"),\n",
    "#     LogisticRegression(penalty='l2', C=1.5, random_state=RANDOM_STATE_VAL,max_iter=120,\n",
    "#                        solver=\"newton-cg\"),\n",
    "#     LogisticRegression(penalty='l2', C=1.5, random_state=RANDOM_STATE_VAL,max_iter=1200,\n",
    "#                        solver=\"newton-cg\"),\n",
    "#     LogisticRegression(penalty='l2', C=1.0, random_state=RANDOM_STATE_VAL,max_iter=2000,\n",
    "#                        solver=\"liblinear\"),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "model_fit_dict = {}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "#     ('MyStandardScaler', MyStandardScaler()),   #標準化\n",
    "     ('MyPreprocessor', MyPreprocessor()),   #独自関数\n",
    "])\n",
    "\n",
    "for model_name, model in model_def_dict.items():\n",
    "    \n",
    "    start = time.time()\n",
    "    print(\"====================== {}  ======================\".format(model_name))\n",
    "    X_train_feature_new = pipeline.fit_transform(X_train_feature, y_train_answer,\n",
    "                                     MyPreprocessor__feature_num = FEATURE_NUM,\n",
    "                                     MyPreprocessor__selector_step = SELECTOR_STEP,\n",
    "                                     MyPreprocessor__model = model,\n",
    "                                    )\n",
    "    x_col_name_list =  X_train_feature_new.columns.tolist()\n",
    "    \n",
    "    \n",
    "    if \"CatBoostClassifier\" in model_name:\n",
    "        ##### CatBoostClassifier #####\n",
    "    #     model.fit(X_train_feature, y_train_answer.values.ravel(),\n",
    "    # #              eval_set=(X_train_validation, y_train_validation.values.ravel()),\n",
    "    #               eval_set=(getAdjustTestColDf(X_train_validation, X_train_feature.columns.tolist()),\n",
    "    #                         y_train_validation.values.ravel()),\n",
    "    #               cat_features=[2,3,4,5,6,7,8,21],\n",
    "    #               plot=True\n",
    "    #              )\n",
    "        model.fit(X_train_feature_new, y_train_answer,\n",
    "                  eval_set=(getAdjustTestColDf(X_train_validation, x_col_name_list),\n",
    "                            y_train_validation),\n",
    "                  cat_features=[x_col_name_list.index(col_name) for col_name in x_col_name_list\n",
    "                                if col_name.endswith(\"_cat\") or col_name.endswith(\"_bin\")],\n",
    "                  plot=True\n",
    "                 )\n",
    "\n",
    "        # ハイパーパラメータの表示\n",
    "        print(\"BestScore = \", model.get_best_score())\n",
    "        print(model.get_all_params())\n",
    "        print(\"最適な木の数 = \",model.tree_count_)\n",
    "        print(model.feature_importances_)\n",
    "    elif \"XGBClassifier\" in model_name or \"LGBMClassifier\" in model_name:\n",
    "        ##### XGBClassifier #####\n",
    "        ##### LGBMClassifier #####\n",
    "        model.fit(X_train_feature_new, y_train_answer)\n",
    "        y_pred=model.predict_proba(getAdjustTestColDf(X_train_validation, x_col_name_list))\n",
    "        print(\"auc_score = \", roc_auc_score(y_train_validation.values.ravel(), \n",
    "                                            y_pred[:,1]))\n",
    "\n",
    "    ### model append ###\n",
    "    model_fit_dict[model_name] =  model\n",
    "print(\"----------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "getAdjustTestColDf(X_train_validation, x_col_name_list)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(y_train_validation)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 重要度が0よりも大きい特徴量に絞る\n",
    "[col_name_dict[index] for index,elem in enumerate(list(model.feature_importances_)) if elem>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_preds = []\n",
    "\n",
    "#### 予測 ####\n",
    "for model_name, model in model_fit_dict.items():\n",
    "    print(\"======================= {} =======================\".format(model_name))\n",
    "    \n",
    "    \n",
    "    if \"CatBoostClassifier\" in model_name:\n",
    "        ##### CatBoostClassifier #####\n",
    "        preds=model.predict(getAdjustTestColDf(act_test_Df_2, X_train_feature_new.columns.tolist()), prediction_type=\"Probability\")\n",
    "    elif \"XGBClassifier\" in model_name or \"LGBMClassifier\" in model_name:\n",
    "        ##### XGBClassifier #####\n",
    "        ##### LGBMClassifier #####\n",
    "        preds=model.predict_proba(getAdjustTestColDf(act_test_Df_2, X_train_feature_new.columns.tolist()))\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(total_preds) > 0:\n",
    "        total_preds = total_preds + preds\n",
    "    else:\n",
    "        total_preds = preds\n",
    "\n",
    "#####\n",
    "# モデル数で割る\n",
    "mean_preds = total_preds / len(model_fit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_Df = pd.DataFrame(\n",
    "    {\n",
    "#        target_col_name: [1 if row[1] >  PROBA_THRESHOLD else 0 for row in  mean_preds] \n",
    "        target_col_name: [row[1] for row in  mean_preds] \n",
    "    }\n",
    "    ,dtype=int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(target_Df, True, 10, \"target_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_Df.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_Df = test_id_Df.join(target_Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(output_Df, True, 10, \"output_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリなければ作る\n",
    "output_dir_path = os.path.join(os.getcwd(), 'output')\n",
    "print(output_dir_path)\n",
    "if not os.path.exists(output_dir_path):\n",
    "    os.makedirs(output_dir_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力\n",
    "output_Df.to_csv('output/submission.csv',\n",
    "                 sep=\",\",\n",
    "                 header=True,\n",
    "                 index=False,\n",
    "                 mode=\"w\",\n",
    "                 encoding=\"utf-8\",\n",
    "                 line_terminator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"END♪\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
