{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C_L_US-CA_ConfirmedCases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "from collections import deque\n",
    "import time\n",
    "\n",
    "import codecs\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "from platform import python_version\n",
    "\n",
    "import shutil\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import csv\n",
    "from platform import python_version\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from imblearn.over_sampling import RandomOverSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大表示列数の指定（ここでは2000列を指定）\n",
    "pd.set_option('display.max_columns', 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大表示行数の指定（ここでは200行を指定）\n",
    "pd.set_option('display.max_rows', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'.[桁数]e'\n",
    "pd.options.display.float_format = '{:10.1f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.options.display.float_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### customize define\n",
    "\n",
    "FEATURE_NUM = 90\n",
    "\n",
    "# 特徴量削減ステップ\n",
    "SELECTOR_STEP = .05\n",
    "\n",
    "#TARGET_PROB_CLASS_VALUE = '1'\n",
    "TARGET_CLASS_VALUE = 1\n",
    "\n",
    "CV_VAL = 5\n",
    "\n",
    "# n_estimators:ここを増やすと精度は上がるが遅い。。。\n",
    "#ESTIMATOR_NUM=5\n",
    "ESTIMATOR_NUM=10\n",
    "#ESTIMATOR_NUM=30\n",
    "#ESTIMATOR_NUM=100\n",
    "\n",
    "\n",
    "# 特徴量選択のアルゴリズムタイプ\n",
    "# RandomForest\n",
    "FEATURE_SELECT_TYPE_RANDOM_FOREST = 1\n",
    "# Logistic\n",
    "FEATURE_SELECT_TYPE_LOGISTIC= 2\n",
    "\n",
    "# 特長量選択に実際に使うアルゴリズムタイプ\n",
    "feature_select_type = FEATURE_SELECT_TYPE_LOGISTIC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラム名変更フラグ\n",
    "COL_NAME_CHG_FLG = True\n",
    "\n",
    "read_train_path = r\"/Users/masayuki/Documents/data/python_workspace/Kaggle/reg/COVID19 Local US-CA/input/ca_train.csv\"\n",
    "\n",
    "read_test_path = r\"/Users/masayuki/Documents/data/python_workspace/Kaggle/reg/COVID19 Local US-CA/input/ca_test.csv\"\n",
    "\n",
    "\n",
    "output_dir_path = r\"/Users/masayuki/Documents/data/python_workspace/Kaggle/reg/COVID19 Local US-CA/output\"\n",
    "\n",
    "pkl_dir_path = r\"/Users/masayuki/Documents/data/python_workspace/Kaggle/reg/COVID19 Local US-CA/pkl\"\n",
    "\n",
    "output_file_name = \"submission.csv\"\n",
    "\n",
    "drop_col_name_list =[\n",
    "    \"Province/State\", \"Country/Region\", \"Lat\", \"Long\"\n",
    "]\n",
    "\n",
    "float_conv_col_name_list = [\n",
    "    \"Date\"\n",
    "]\n",
    "\n",
    "float_nan_conv_dic ={\n",
    "    # name: [min, max]\n",
    "}\n",
    "\n",
    "# その他独自変換処理用カラムリスト\n",
    "other_conv_process = [\n",
    "    \"Date\"\n",
    "]\n",
    "\n",
    "\n",
    "id_col_name=\"ForecastId\"\n",
    "target_col_name = \"ConfirmedCases\"\n",
    "\n",
    "RANDOM_STATE_VAL = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispalyDf(readDf, dispFlag, row_num, dispLabel):\n",
    "    print(\"========================= {} =========================\".format(dispLabel))\n",
    "    row_num , col_num = np.shape(readDf)\n",
    "    print(\"row_num = {}, col_num = {}\".format(row_num, col_num))\n",
    "    if dispFlag:\n",
    "        display(readDf.head(row_num))\n",
    "    print(\"++++++ None Count All++++++\")\n",
    "    print(readDf.isnull().values.sum())\n",
    "    print(\"++++++ None Count ++++++\")\n",
    "    print(readDf.isnull().sum())\n",
    "    print(\"++++++ Col Type ++++++\")\n",
    "    print(readDf.dtypes)\n",
    "#     print(\"++++++ Unique By Col（Noneは含まないので注意） ++++++\")\n",
    "#     for col_name in readDf.columns.tolist():\n",
    "#         print(\"{} : {}\".format(col_name, readDf[col_name].nunique()))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chkNum(expect_label, expect_num, act_label, act_num):\n",
    "    if expect_num != act_num:\n",
    "        raise Exception(\"Num Error !! expect({}): {}, act({}) : {}\"\n",
    "                        .format(expect_label, expect_num, act_label, act_num))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardScalerDf(inputDf):\n",
    "    row_num, col_num = np.shape(inputDf)\n",
    "    \n",
    "    # 平準化\n",
    "    scaler = StandardScaler()\n",
    "    val_array = scaler.fit_transform(np.array(inputDf.values.tolist()))\n",
    "    standardScDf = pd.DataFrame(val_array, dtype=np.float, columns = inputDf.columns.tolist())\n",
    "    \n",
    "    sc_row_num, sc_col_num = np.shape(standardScDf)\n",
    "\n",
    "    # 行数チェック\n",
    "    chkNum(\"expect_row_num\", row_num, \"sc_row_num\", sc_row_num)\n",
    "    # 列数チェック\n",
    "    chkNum(\"expect_col_num\", col_num, \"sc_col_num\", sc_col_num)\n",
    "\n",
    "    \n",
    "    del val_array\n",
    "    return standardScDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test用のカラムのDataFrame\n",
    "def getAdjustTestColDf(inputDf, train_col_list):\n",
    "    not_enough_col_list = set(train_col_list) - set(inputDf.columns.tolist())\n",
    "    print(\"# trainに有ってtestにないカラム名のリスト\")\n",
    "    print(\"{} : {}\".format(len(not_enough_col_list), not_enough_col_list))\n",
    "        \n",
    "    dataLen = len(inputDf)\n",
    "        \n",
    "    retDf = inputDf.join(\n",
    "        pd.DataFrame({col_name : [0]*dataLen  for col_name  in not_enough_col_list },\n",
    "                     dtype=np.float, columns=not_enough_col_list)\n",
    "    ).loc[:, train_col_list]\n",
    "    \n",
    "    # カラムリストが一致するかチェック\n",
    "    if retDf.columns.tolist() != train_col_list:\n",
    "        raise Exception(\"col_name_list not Match train and test.\")\n",
    "\n",
    "    # データ行数が一致するかチェック\n",
    "    chkNum(\"dataLen\", dataLen, \"retDf\", len(retDf))\n",
    "\n",
    "    return retDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測\n",
    "def getPredictDf(model, test_Df, id_Df , target_col_name):\n",
    "    return id_Df.join(\n",
    "            pd.DataFrame(model.predict(test_Df),\n",
    "                         dtype=np.int, columns=[target_col_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "####################################\n",
    "##### 次元削減(特徴量選択) を使用する\n",
    "##### ↓特徴量選択(LogisticRegression)\n",
    "####################################\n",
    "def getSelectorFeature(inputFeatureDf,y_Df,feature_num, feature_select_type , selector_step):\n",
    "\n",
    "    # 特徴量因子の重要度を推定する分類器を決定する\n",
    "    # 0未満 の場合はNoneに設定する。\n",
    "    # 1回のstepで削除する次元数は5%ずつとする\n",
    "    if feature_select_type == FEATURE_SELECT_TYPE_RANDOM_FOREST:\n",
    "        sel_algo = \"RandomForestClassifier\"\n",
    "        est_num = 10\n",
    "        estimator_algo = RandomForestClassifier(n_estimators=est_num, random_state=RANDOM_STATE_VAL)\n",
    "        pkl_suffix = \"est{}\".format(est_num)\n",
    "    elif feature_select_type == FEATURE_SELECT_TYPE_LOGISTIC:\n",
    "        sel_algo = \"LogisticRegression\"\n",
    "        maxiter_num = 120\n",
    "        C_num = 1.5\n",
    "        estimator_algo = LogisticRegression(penalty='l2', C=C_num, random_state=RANDOM_STATE_VAL, max_iter=maxiter_num,\n",
    "                                           solver='liblinear')\n",
    "        pkl_suffix = \"maxiter{}_C{}\".format(maxiter_num, C_num)\n",
    "    else:\n",
    "        raise Exception(\"feature_select_type Error!! {}\".format(feature_select_type))\n",
    "    print(\"========================== {} ==========================\".format(sel_algo))\n",
    "    \n",
    "    \n",
    "    print(\"特徴量選択数 = \",feature_num)\n",
    "    if(feature_num < 0):\n",
    "        # デフォルト値\n",
    "        print(\"n_features_to_select = None に設定\")\n",
    "        feature_num = None\n",
    "    elif feature_num >= len(inputFeatureDf.columns.tolist()):\n",
    "        raise Exception(\"feature_num({}) >= colnum({})\".format(feature_num, len(inputFeatureDf.columns.tolist())))\n",
    "    \n",
    "    # 特徴量選択\n",
    "    # step:特徴量削除の速度。一度の再帰処理により指定ステップ分の特徴量が消滅する。\n",
    "    selector = RFE(estimator=estimator_algo,\n",
    "                   n_features_to_select=feature_num,\n",
    "                   step=selector_step)\n",
    "    selector.fit(inputFeatureDf,y_Df.values.ravel())\n",
    "    #selector.fit(inputFeatureDf, y_Df.iloc[:,0])\n",
    "\n",
    "    # 削除対象配列(Falseが削除対象)\n",
    "    selFlagArray = selector.support_\n",
    "    \n",
    "    # 現状のカラム名を取得\n",
    "    columnIndex = inputFeatureDf.columns\n",
    "    \n",
    "    # 削除対象カラム名リスト\n",
    "    drop_column_list = list()\n",
    "    # 削除対象配列 でループ(i=0,1,2,・・・)\n",
    "    print(len(selFlagArray))\n",
    "    for i in range(len(selFlagArray)):\n",
    "        # Falseの場合に、削除対象リストに追加\n",
    "        if(selFlagArray[i] == False):\n",
    "            drop_column_list.append(columnIndex[i])\n",
    "\n",
    "    # 不要な列削除\n",
    "    #print(\"削除対象カラム = \" , drop_column_list)\n",
    "    inputFeatureNewDf = inputFeatureDf.drop(drop_column_list, axis=1)\n",
    "    \n",
    "\n",
    "    return inputFeatureNewDf , \"train_{}_feature{}_{}.pkl\".format(sel_algo, feature_num, pkl_suffix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# その他変換処理を行う。\n",
    "def  otherConvProcess(convDf, other_conv_process):\n",
    "    dateime_col_name = other_conv_process[0]\n",
    "    date_val_list = convDf[dateime_col_name].values.tolist()\n",
    "    conv_val_list = [datetime.strptime(elem, '%Y-%m-%d').timestamp() for elem in date_val_list]\n",
    "    convDf[dateime_col_name] = pd.Series(conv_val_list)\n",
    "    dispalyDf(convDf, True, 5,  \"convDf\")\n",
    "\n",
    "    return convDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理関数\n",
    "def getBeforeProcessDf(inputDf, drop_col_name_list, float_conv_col_name_list, float_nan_conv_dic,\n",
    "                       train_Flg, train_target_Df ,\n",
    "                       random_state_val, feature_num, \n",
    "                       feature_select_type, \n",
    "                       selector_step, train_col_list, other_conv_process):\n",
    "    # inputをコピーしておく\n",
    "    convDf = inputDf.copy()\n",
    "    \n",
    "    # 列削除\n",
    "    convDf = convDf.drop(drop_col_name_list, axis=1)\n",
    "    \n",
    "    # カラム名リスト保持\n",
    "    col_list = convDf.columns.tolist()\n",
    "    \n",
    "    # その他変換処理\n",
    "    convDf = otherConvProcess(convDf, other_conv_process)\n",
    "    \n",
    "\n",
    "    print(\"連続値のカラムの欠損値補完 = \",datetime.now())\n",
    "    # 連続値のカラムの欠損値補完\n",
    "    float_conv_dic={}\n",
    "    for col_name in float_conv_col_name_list:\n",
    "        float_values_list = [np.float(elem)  for elem in convDf[col_name].values.tolist()]\n",
    "        \n",
    "        if len(float_nan_conv_dic) > 0:\n",
    "            conv_list = \\\n",
    "            [(\n",
    "                np.random.randint(\n",
    "                    float_nan_conv_dic[col_name][0],\n",
    "                    float_nan_conv_dic[col_name][1])\n",
    "                if col_name in float_nan_conv_dic else 0\n",
    "            if np.isnan(elem) else elem )\n",
    "             for elem in float_values_list]\n",
    "            \n",
    "            float_conv_dic[col_name]=conv_list\n",
    "            del conv_list\n",
    "        else:\n",
    "            float_conv_dic[col_name]=float_values_list\n",
    "        \n",
    "    print(\"dummy = \",datetime.now())\n",
    "    ##### float変換がある場合 ####\n",
    "    if len(float_conv_dic)>0:\n",
    "        # DataFrameを起こす。\n",
    "        float_conv_df = pd.DataFrame(float_conv_dic,dtype=np.float)\n",
    "    \n",
    "        #dispalyDf(float_conv_df, True, 5,  \"float_conv_df\")\n",
    "\n",
    "        col_list_except_float = [col_name for col_name in col_list \n",
    "                                 if col_name not in float_conv_col_name_list]\n",
    "\n",
    "        # float以外のカラムが存在する場合\n",
    "        if len(col_list_except_float) > 0:\n",
    "            # float以外のDataFrameにする\n",
    "            convDf = convDf.loc[:,col_list_except_float]\n",
    "\n",
    "            # NaNも一つのカテゴリーとしてダミー変数化したい場合は、引数dummy_na=Trueとする。\n",
    "            convDf = pd.get_dummies(convDf, dummy_na=True, columns=col_list_except_float)\n",
    "\n",
    "            #  全ての特徴量\n",
    "            allFeatureDf = convDf.join(float_conv_df)\n",
    "            \n",
    "        else:\n",
    "            # float以外のカラムが存在しない場合\n",
    "            allFeatureDf = float_conv_df\n",
    "\n",
    "            \n",
    "    ##### float変換がない場合 ####\n",
    "    else:\n",
    "        # NaNも一つのカテゴリーとしてダミー変数化したい場合は、引数dummy_na=Trueとする。\n",
    "        #  全ての特徴量\n",
    "        allFeatureDf = pd.get_dummies(convDf, dummy_na=True, columns=col_list)\n",
    "\n",
    "    ###\n",
    "    #dispalyDf(allFeatureDf, True, 5,  \"allFeatureDf\")\n",
    "\n",
    "    if train_Flg:\n",
    "        # trainの場合\n",
    "        # 標準化\n",
    "#        print(\"標準化 = \",datetime.now())\n",
    "#        allFeatureDf =  standardScalerDf(allFeatureDf)\n",
    "\n",
    "\n",
    "#         # Ridge で特徴量選択を行う。\n",
    "#         print(\"特徴量選択 = \",datetime.now())\n",
    "#         retFeatureDf , retPklFile = getSelectorFeature(allFeatureDf,  train_target_Df, feature_num,\n",
    "#                                                        feature_select_type, selector_step)\n",
    "\n",
    "        retFeatureDf = allFeatureDf\n",
    "        retPklFile = \"train_feature_ConfirmedCases.pkl\"\n",
    "        \n",
    "    else:\n",
    "        # testの場合\n",
    "        # カラムをtrainと合わせる\n",
    "        print(\"カラム調整 = \",datetime.now())\n",
    "        testFeatureDf = getAdjustTestColDf(allFeatureDf, train_col_list)\n",
    "#         # 標準化\n",
    "#         print(\"標準化 = \",datetime.now())\n",
    "#         testFeatureDf =  standardScalerDf(testFeatureDf)\n",
    "        \n",
    "        retFeatureDf = testFeatureDf\n",
    "        retPklFile = \"\"\n",
    "        \n",
    "    \n",
    "    print(\"end = \",datetime.now())\n",
    "    return retFeatureDf , retPklFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル読み込み(train)\n",
    "train_read_Df = pd.read_csv(read_train_path,\n",
    "                            header=0,\n",
    "                            encoding=\"utf-8\",\n",
    "                            sep='\\n',\n",
    "                            delimiter=',',\n",
    "                            engine='python',\n",
    "                            dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル読み込み(test)\n",
    "test_read_Df = pd.read_csv(read_test_path,\n",
    "                           header=0,\n",
    "                           encoding=\"utf-8\",\n",
    "                           sep='\\n',\n",
    "                           delimiter=',',\n",
    "                           engine='python',\n",
    "                           dtype=object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf = train_read_Df.copy()\n",
    "testDf = test_read_Df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### カラム名変更処理　#######\n",
    "if COL_NAME_CHG_FLG:\n",
    "    trainDf = trainDf.rename(columns={\"Id\": id_col_name})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(trainDf, True, 5,  \"trainDf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(testDf, True, 5,  \"testDf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラムの差分\n",
    "set(trainDf.columns.tolist()) - set(testDf.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetのユニーク値\n",
    "trainDf[target_col_name].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 独自処理\n",
    "print(trainDf[\"Province/State\"].nunique())\n",
    "print(trainDf[\"Country/Region\"].nunique())\n",
    "print(trainDf[\"Lat\"].nunique())\n",
    "print(trainDf[\"Long\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testDf[\"Province/State\"].nunique())\n",
    "print(testDf[\"Country/Region\"].nunique())\n",
    "print(testDf[\"Lat\"].nunique())\n",
    "print(testDf[\"Long\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train:データのDataFrameと、ターゲットのデータフレームに分ける。\n",
    "train_target_Df = trainDf.loc[:,[target_col_name]]\n",
    "trainDf = trainDf.loc[:,[col_name for col_name in trainDf.columns.tolist() if col_name not in [id_col_name,target_col_name]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:idとデータのDataFrameを抽出。\n",
    "test_id_Df = testDf.loc[:,[id_col_name]]\n",
    "testDf = testDf.loc[:,[col_name for col_name in testDf.columns.tolist() if col_name != id_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train のデータ前処理\n",
    "train_new_Df, train_pkl_file = getBeforeProcessDf(\n",
    "    trainDf, drop_col_name_list + [\"Fatalities\"],\n",
    "    float_conv_col_name_list, float_nan_conv_dic,\n",
    "    True , train_target_Df,\n",
    "    RANDOM_STATE_VAL, FEATURE_NUM,\n",
    "    feature_select_type,\n",
    "    SELECTOR_STEP, None, other_conv_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(train_new_Df, True, 5,  \"train_new_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理が終わったtrainのpklを保存\n",
    "with open(os.path.join(pkl_dir_path, train_pkl_file), 'wb') as DFfile:\n",
    "    pickle.dump(train_new_Df, DFfile, protocol=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test のデータ前処理\n",
    "test_new_Df, _ = getBeforeProcessDf(\n",
    "    testDf, drop_col_name_list,\n",
    "    float_conv_col_name_list, float_nan_conv_dic,\n",
    "    False , None,\n",
    "    None, None,\n",
    "    None,\n",
    "    None, train_new_Df.columns.tolist(), other_conv_process)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(test_new_Df, True, 5,  \"test_new_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを増やす。不均衡調整。\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_over,y_over = ros.fit_sample(train_new_Df, train_target_Df.values.ravel())\n",
    "print(len(X_over))\n",
    "print(len(y_over))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_new_Df = pd.DataFrame(X_over,dtype=np.float,columns=[\"Date\"])\n",
    "train_target_Df = pd.DataFrame(y_over,dtype=np.float,columns=[target_col_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(train_new_Df, True, 5,  \"train_new_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(train_target_Df, True, 5,  \"train_target_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル入手\n",
    "def getModel(train_Df, y_train_Df, random_state_val, cv_val):\n",
    "    print(\"cv_val = \",cv_val)\n",
    "    model_def_list = [\n",
    "        [\n",
    "            \"Ridge\",\n",
    "            Ridge(random_state=random_state_val, alpha=1.3)\n",
    "        ],\n",
    "        [\n",
    "            \"OLS\",\n",
    "            LinearRegression()\n",
    "        ],\n",
    "        [\n",
    "            \"SGDRegressor\",\n",
    "            SGDRegressor(random_state=random_state_val, alpha=0.3, max_iter=55000, eta0=0.0005)\n",
    "        ]\n",
    "    \n",
    "    ]\n",
    "    \n",
    "    \n",
    "    ret_model_list = []\n",
    "    test_score_list = []\n",
    "    \n",
    "    \n",
    "    for model_elem in model_def_list:\n",
    "        print(\" ==================== {} ====================\".format(model_elem[0]))\n",
    "        model = model_elem[1]\n",
    "        start = time.time()\n",
    "        model.fit(train_Df, y_train_Df.values.ravel())\n",
    "        ret_model_list.append(model)\n",
    "\n",
    "        cv_results = cross_validate(model,\n",
    "                                                         train_Df,\n",
    "                                                         y_train_Df.values.ravel(),      #1次元のARRAY\n",
    "                                                         cv=cv_val,\n",
    "                                                         scoring='r2')\n",
    "#         print(cv_results)\n",
    "#         print(cv_results[\"test_score\"])\n",
    "        print(\"test_score = \", np.mean(cv_results[\"test_score\"]))\n",
    "        test_score_list.append(np.mean(cv_results[\"test_score\"]))\n",
    "\n",
    "        elapsed_time = time.time() - start\n",
    "        print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    # test_score が最大のアルゴリズムを適用する\n",
    "    print(test_score_list)\n",
    "    selected_index = np.argmax(test_score_list)\n",
    "    print(\"selected_model : {}\".format(model_def_list[selected_index][0]))\n",
    "    return ret_model_list[selected_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル入手\n",
    "model = getModel(train_new_Df, train_target_Df, RANDOM_STATE_VAL, CV_VAL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測\n",
    "resultDf =getPredictDf(model, test_new_Df, test_id_Df , target_col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(resultDf, True, 5,  \"resultDf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリなければ作る\n",
    "if not os.path.exists(output_dir_path):\n",
    "    os.makedirs(output_dir_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ouput_file_path = os.path.join(output_dir_path, output_file_name)\n",
    "print(ouput_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイル出力\n",
    "resultDf.to_csv(ouput_file_path,\n",
    "               sep=\",\",\n",
    "                header=True,\n",
    "                index=False,\n",
    "                mode=\"w\",\n",
    "                encoding=\"utf-8\",\n",
    "                line_terminator=\"\\n\"\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"END♪\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
