{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# santander-value-prediction-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import os\n",
    "from collections import deque\n",
    "from collections import Counter\n",
    "import codecs\n",
    "import copy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "import time\n",
    "import shutil\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import csv\n",
    "import math\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.metrics import roc_auc_score, f1_score\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier, VotingClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "from plotly import subplots\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大表示列数の指定（ここでは5000列を指定）\n",
    "pd.set_option('display.max_columns', 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#最大表示行数の指定（ここでは200行を指定）\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_estimators:ここを増やすと精度は上がるが遅い。。。\n",
    "ESTIMATOR_NUM=10\n",
    "RANDOM_STATE_VAL = 0\n",
    "target_col_name = \"target\"\n",
    "id_col_name = \"ID\"\n",
    "drop_columns =  []\n",
    "\n",
    "#FEATURE_NUM = 2000\n",
    "#FEATURE_NUM = 1500\n",
    "#FEATURE_NUM = 1250\n",
    "#FEATURE_NUM = 1000\n",
    "#FEATURE_NUM = 950\n",
    "#FEATURE_NUM = 900\n",
    "#FEATURE_NUM = 800\n",
    "#FEATURE_NUM = 500\n",
    "#FEATURE_NUM = 100\n",
    "#FEATURE_NUM = 90\n",
    "#FEATURE_NUM = 50\n",
    "FEATURE_NUM = 40\n",
    "\n",
    "# 特徴量削減ステップ\n",
    "SELECTOR_STEP = .05\n",
    "\n",
    "CV_VAL = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotDistribution(rowcount_tuple, target_value, title = '') :\n",
    "    plotObj = subplots.make_subplots(rows=1, cols=2, \n",
    "                                     subplot_titles=('Scatter', 'Histogram',),\n",
    "                                    )\n",
    "    plotObj.append_trace(go.Scatter(x=rowcount_tuple, y=np.sort(target_value), mode='lines', connectgaps=True,),\n",
    "                            1,1,\n",
    "                           )\n",
    "    plotObj.append_trace(go.Histogram(x=target_value), \n",
    "                            1,2,\n",
    "                           )\n",
    "    plotObj['layout'].update(title = title, titlefont = dict(family = 'Arial', size = 36,), \n",
    "                                paper_bgcolor = '#d3b8d7', plot_bgcolor = '#71e1df',\n",
    "                               )\n",
    "    return py.iplot(plotObj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dispalyDf(readDf, dispFlag, row_num, dispLabel):\n",
    "    print(\"========================= {} =========================\".format(dispLabel))\n",
    "    row_num , col_num = np.shape(readDf)\n",
    "    print(\"row_num = {}, col_num = {}\".format(row_num, col_num))\n",
    "    if dispFlag:\n",
    "        display(readDf.head(row_num))\n",
    "    print(\"++++++ None Count All++++++\")\n",
    "    print(readDf.isnull().values.sum())\n",
    "    print(\"++++++ None Count ++++++\")\n",
    "    print(readDf.isnull().sum())\n",
    "    print(\"++++++ Col Type ++++++\")\n",
    "    print(readDf.dtypes)\n",
    "#     print(\"++++++ Unique By Col（Noneは含まないので注意） ++++++\")\n",
    "#     for col_name in readDf.columns.tolist():\n",
    "#         print(\"{} : {}\".format(col_name, readDf[col_name].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chkVal(expect_label, expect_val, act_label, act_val):\n",
    "    if expect_val != act_val:\n",
    "        raise Exception(\"Val Error !! expect({}): {}, act({}) : {}\"\n",
    "                        .format(expect_label, expect_val, act_label, act_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test用のカラムのDataFrame\n",
    "def getAdjustTestColDf(inputDf, train_col_list):\n",
    "    not_enough_col_list = set(train_col_list) - set(inputDf.columns.tolist())\n",
    "    print(\"# trainに有ってtestにないカラム名のリスト\")\n",
    "    print(\"{} : {}\".format(len(not_enough_col_list), not_enough_col_list))\n",
    "        \n",
    "    dataLen = len(inputDf)\n",
    "        \n",
    "    retDf = inputDf.join(\n",
    "        pd.DataFrame({col_name : [0]*dataLen  for col_name  in not_enough_col_list },\n",
    "                     dtype=np.float, columns=not_enough_col_list)\n",
    "    ).loc[:, train_col_list]\n",
    "    \n",
    "    # カラムリストが一致するかチェック\n",
    "    if retDf.columns.tolist() != train_col_list:\n",
    "        raise Exception(\"col_name_list not Match train and test.\")\n",
    "\n",
    "    # データ行数が一致するかチェック\n",
    "    chkVal(\"dataLen\", dataLen, \"retDf\", len(retDf))\n",
    "\n",
    "    return retDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getScalerDf(inputDf):\n",
    "    row_num, col_num = np.shape(inputDf)\n",
    "    \n",
    "    # 平準化\n",
    "    #scaler = StandardScaler()\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    val_array = scaler.fit_transform(np.array(inputDf.values.tolist()))\n",
    "    standardScDf = pd.DataFrame(val_array, dtype=np.float, columns = inputDf.columns.tolist())\n",
    "    \n",
    "    sc_row_num, sc_col_num = np.shape(standardScDf)\n",
    "\n",
    "    # 行数チェック\n",
    "    chkVal(\"expect_row_num\", row_num, \"sc_row_num\", sc_row_num)\n",
    "    # 列数チェック\n",
    "    chkVal(\"expect_col_num\", col_num, \"sc_col_num\", sc_col_num)\n",
    "\n",
    "    \n",
    "    del val_array\n",
    "    return standardScDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNplogDf(inputDf):\n",
    "    return np.log1p(inputDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDeconvNplogDf(inputData):\n",
    "    return np.exp(inputData) - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_read_Df = pd.read_csv('input/train.csv')\n",
    "act_test_read_Df = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df = act_train_read_Df.copy()\n",
    "act_test_Df = act_test_read_Df.copy()\n",
    "del act_train_read_Df, act_test_read_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dispalyDf(act_train_Df, True, 10, \"act_train_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dispalyDf(act_test_Df, True, 10, \"act_test_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train:データのDataFrameと、ターゲットのデータフレームに分ける。\n",
    "train_target_Ser = act_train_Df[target_col_name]\n",
    "act_train_Df = act_train_Df.loc[:,[col_name for col_name in act_train_Df.columns.tolist() if col_name not in [id_col_name,target_col_name]]]\n",
    "#print(set(train_target_Ser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test:idとデータのDataFrameを抽出。\n",
    "test_id_Df = act_test_Df.loc[:,[id_col_name]]\n",
    "act_test_Df = act_test_Df.loc[:,[col_name for col_name in act_test_Df.columns.tolist() if col_name != id_col_name]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# targetの分布を調べる\n",
    "# \n",
    "plotDistribution(tuple(range(act_train_Df.shape[0])), train_target_Ser, 'Target')\n",
    "\n",
    "# logを取る\n",
    "plotDistribution(tuple(range(act_train_Df.shape[0])), np.log1p(train_target_Ser), 'Target - log1p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 値のユニーク数が１のカラムをdropリストに追加する\n",
    "drop_columns.extend([col_name for col_name in act_train_Df.columns.tolist()\n",
    "                    if act_train_Df[col_name].nunique() == 1 not in drop_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convDescFeature(inputDf):\n",
    "    retDf = inputDf.copy()\n",
    "    \n",
    "    retDf['quantile'] = \\\n",
    "        inputDf[inputDf > 0].quantile(0.5, axis=1)\n",
    "    retDf['max'] = \\\n",
    "        inputDf.max(axis=1)\n",
    "    retDf['skew'] = \\\n",
    "        inputDf.skew(axis=1)\n",
    "    retDf['kurtosis'] = \\\n",
    "        inputDf.kurtosis(axis=1)\n",
    "    retDf['sum_all'] = \\\n",
    "        inputDf.sum(axis=1)\n",
    "    retDf['variance'] = \\\n",
    "        inputDf.var(axis=1)\n",
    "    \n",
    "    retDf = retDf.loc[:, [\"quantile\", \"max\", \"skew\", \"kurtosis\", \"sum_all\", \"variance\"]] \n",
    "    \n",
    "    return retDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### 前処理 ######\n",
    "def getBeroreProcessDf(input_Df, drop_columns):\n",
    "    retDf = input_Df.copy()\n",
    "    retDf = retDf.drop(drop_columns, axis=1)\n",
    "    \n",
    "    columns = retDf.columns.tolist()\n",
    "    \n",
    "    \n",
    "    for col_name in columns:\n",
    "        if retDf[col_name].dtype == object:\n",
    "            # 文字列のカラムの加工処理\n",
    "            pass\n",
    "#            retDf[col_name] = retDf[col_name].fillna('0')\n",
    "#            retDf[col_name] = retDf[col_name].apply(lambda x: x.split(' ')[1])\n",
    "#         # int64に変換する。\n",
    "#         retDf[col_name] = pd.to_numeric(retDf[col_name])\n",
    "\n",
    "    # 0→nan\n",
    "    retDf = retDf.replace(0, np.nan)\n",
    "\n",
    "#    retDf =  getScalerDf(retDf)\n",
    "    retDf =  getNplogDf(retDf)\n",
    "\n",
    "    #### desc特徴量に変換 #####\n",
    "    retDf = convDescFeature(retDf)\n",
    "\n",
    "    # 0←nan\n",
    "    retDf = retDf.replace(np.nan, 0)\n",
    "\n",
    "    return retDf\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### trainの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_train_Df_2 = getBeroreProcessDf(act_train_Df, drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(act_train_Df_2, True, 10, \"act_train_Df_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### testの前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_test_Df_2 = getBeroreProcessDf(act_test_Df, drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(act_test_Df_2, True, 10, \"act_test_Df_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log1p(train_target_Ser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "##### 次元削減(特徴量選択) を使用する\n",
    "##### ↓特徴量選択(Ridge)\n",
    "####################################\n",
    "def getSelectorFeature(inputFeatureDf, model, y_Df,feature_num, selector_step):\n",
    "    # 特徴量因子の重要度を推定する分類器をRidgeに設定\n",
    "    # 1回のstepで削除する次元数は5%ずつとする\n",
    "    \n",
    "    print(\"特徴量選択数 = \",feature_num)\n",
    "    if(feature_num < 0):\n",
    "        # デフォルト値\n",
    "        print(\"n_features_to_select = None に設定\")\n",
    "        feature_num = None\n",
    "    elif feature_num > len(inputFeatureDf.columns.tolist()):\n",
    "        print (\"feature_num({}) is Over colnum({})\".format(feature_num, len(inputFeatureDf.columns.tolist())))\n",
    "        # 次元削減せずに返却する\n",
    "        return inputFeatureDf\n",
    "    \n",
    "    # 特徴量選択\n",
    "    # step:特徴量削除の速度。一度の再帰処理により指定ステップ分の特徴量が消滅する。\n",
    "#    selector = RFE(estimator=Ridge(random_state=RANDOM_STATE_VAL, alpha=1.3),\n",
    "    selector = RFE(estimator=model,\n",
    "                   n_features_to_select=feature_num,\n",
    "                   step=selector_step)\n",
    "    selector.fit(inputFeatureDf,y_Df.values.ravel())\n",
    "    #selector.fit(inputFeatureDf, y_Df.iloc[:,0])\n",
    "\n",
    "    # 削除対象配列(Falseが削除対象)\n",
    "    selFlagArray = selector.support_\n",
    "    \n",
    "    # 現状のカラム名を取得\n",
    "    columnIndex = inputFeatureDf.columns\n",
    "    \n",
    "    # 削除対象カラム名リスト\n",
    "    drop_column_list = list()\n",
    "    # 削除対象配列 でループ(i=0,1,2,・・・)\n",
    "    print(len(selFlagArray))\n",
    "    for i in range(len(selFlagArray)):\n",
    "        # Falseの場合に、削除対象リストに追加\n",
    "        if(selFlagArray[i] == False):\n",
    "            drop_column_list.append(columnIndex[i])\n",
    "\n",
    "    # 不要な列削除\n",
    "    #print(\"削除対象カラム = \" , drop_column_list)\n",
    "    inputFeatureNewDf = inputFeatureDf.drop(drop_column_list, axis=1)\n",
    "    \n",
    "\n",
    "    return inputFeatureNewDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "##### 次元削減(nuniqueによる特徴量選択) を使用する\n",
    "####################################\n",
    "def getSelectorFeatureByNunique(inputFeatureDf, feature_num):\n",
    "    temp_dict = {\n",
    "        \"col_name\":[],\n",
    "        \"nunique\":[]\n",
    "    }\n",
    "    for col_name in inputFeatureDf.columns.tolist():\n",
    "        temp_dict[\"col_name\"].append(col_name)\n",
    "        temp_dict[\"nunique\"].append(inputFeatureDf[col_name].nunique())\n",
    "\n",
    "    rankDf = pd.DataFrame(temp_dict)\n",
    "\n",
    "    rankDf['順位'] = (rankDf.rank(method='min',ascending=False)[\"nunique\"]).astype('int')\n",
    "    rankDf = rankDf.sort_values('順位', ascending=True)\n",
    "    rankDf = rankDf[rankDf[\"順位\"] <= feature_num]\n",
    "    \n",
    "    tempDf = inputFeatureDf.loc[:, rankDf[\"col_name\"].values.tolist()]\n",
    "    \n",
    "    return tempDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル入手\n",
    "def getModel(train_Df, y_train, random_state_val, cv_val, feature_num, selector_step):\n",
    "    print(\"cv_val = \",cv_val)\n",
    "    model_def_list = [\n",
    "        [\n",
    "            \"Ridge\",\n",
    "             #Ridge(random_state=random_state_val, alpha=100)\n",
    "             #Ridge(random_state=random_state_val, alpha=20)\n",
    "            Ridge(random_state=random_state_val, alpha=10)\n",
    "             #Ridge(random_state=random_state_val, alpha=100,tol=100000)\n",
    "            #Ridge(random_state=random_state_val, alpha=1.3)\n",
    "            #Ridge(random_state=random_state_val)\n",
    "        ],\n",
    "        [\n",
    "            \"OLS\",\n",
    "            LinearRegression()\n",
    "        ],\n",
    "        [\n",
    "            \"SGDRegressor\",\n",
    "#            SGDRegressor(random_state=random_state_val, alpha=0.3)\n",
    "            SGDRegressor(random_state=random_state_val)\n",
    "        ]\n",
    "    \n",
    "    ]\n",
    "    \n",
    "    \n",
    "    ret_model_list = []\n",
    "    test_score_list = []\n",
    "    \n",
    "    \n",
    "    for model_elem in model_def_list:\n",
    "        print(\" ==================== {} ====================\".format(model_elem[0]))\n",
    "        model = model_elem[1]\n",
    "        \n",
    "        start = time.time()\n",
    "        # 特徴量選択\n",
    "#        train_Df_new = getSelectorFeature(train_Df, model, y_train, feature_num, selector_step)\n",
    "#        train_Df_new = getSelectorFeatureByNunique(train_Df, feature_num)\n",
    "        train_Df_new = train_Df\n",
    "\n",
    "        \n",
    "        model.fit(train_Df_new, y_train.values.ravel())\n",
    "        ret_model_list.append(model)\n",
    "\n",
    "        cv_results = cross_validate(model,\n",
    "                                                         train_Df_new,\n",
    "                                                         y_train.values.ravel(),      #1次元のARRAY\n",
    "                                                         cv=cv_val,\n",
    "                                                         scoring='r2')\n",
    "#         print(cv_results)\n",
    "#         print(cv_results[\"test_score\"])\n",
    "        print(\"test_score = \", np.mean(cv_results[\"test_score\"]))\n",
    "        test_score_list.append(np.mean(cv_results[\"test_score\"]))\n",
    "\n",
    "        elapsed_time = time.time() - start\n",
    "        print (\"elapsed_time:{0}\".format(elapsed_time) + \"[sec]\")\n",
    "    \n",
    "    print(\"----------------------------------------------------------------------------------------\")\n",
    "    # test_score が最大のアルゴリズムを適用する\n",
    "    print(test_score_list)\n",
    "    selected_index = np.argmax(test_score_list)\n",
    "    print(\"selected_model : {}\".format(model_def_list[selected_index][0]))\n",
    "    return ret_model_list[selected_index],  train_Df_new.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル入手\n",
    "# model, train_col_name_list = getModel(act_train_Df_2, train_target_Ser, RANDOM_STATE_VAL, CV_VAL,\n",
    "#                                       FEATURE_NUM, SELECTOR_STEP)\n",
    "model, train_col_name_list = getModel(act_train_Df_2, np.log1p(train_target_Ser), RANDOM_STATE_VAL, CV_VAL,\n",
    "                                     FEATURE_NUM, SELECTOR_STEP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_test_Df_3 = getAdjustTestColDf(act_test_Df_2, train_col_name_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test間でカラムが一致しているかチェック\n",
    "chkVal(\"train_col_list\", train_col_name_list, \"test_col_list\", act_test_Df_3.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(act_test_Df_3, True, 10, \"act_test_Df_3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 予測 ####\n",
    "test_predict = model.predict(act_test_Df_3)\n",
    "target_Df = pd.DataFrame(\n",
    "    {\n",
    "        target_col_name: test_predict \n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getDeconvNplogDf(target_Df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_Df = test_id_Df.join(getDeconvNplogDf(target_Df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_Df[output_Df[target_col_name] < 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dispalyDf(output_Df, True, 10, \"output_Df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ディレクトリなければ作る\n",
    "output_dir_path = os.path.join(os.getcwd(), 'output')\n",
    "print(output_dir_path)\n",
    "if not os.path.exists(output_dir_path):\n",
    "    os.makedirs(output_dir_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 出力\n",
    "output_Df.to_csv('output/submission.csv',\n",
    "                 sep=\",\",\n",
    "                 header=True,\n",
    "                 index=False,\n",
    "                 mode=\"w\",\n",
    "                 encoding=\"utf-8\",\n",
    "                 line_terminator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"END♪\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
