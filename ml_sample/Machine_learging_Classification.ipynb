{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# サンプル(Classification)¶\n",
    "(C)  2018-2019 Masayuki Isobe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●はじめに\n",
    "\n",
    "・初回実行前は、./data/pkl/配下は空にしておいてください。<br>\n",
    "\n",
    "・./config.iniについて、実行モード・pkl自動削除モードが0になっていることを確認してください。<br>\n",
    "　提出用ファイル名・特徴量数・事前評価の方法・cross validationの回数・予測確率カラム名 は、<br>\n",
    "　変える必要がある場合は変えてください。\n",
    "\n",
    "・./data/フォルダ配下に訓練用データ と 検証用データを格納願います。<BR>\n",
    "　さらに、UTF-8、CRLFに変換願います。\n",
    "\n",
    "・OneHotEncoder対象のカラム名を、\"data/OneHotEncoderColumns.dat\"に1行ずつに分けてUTF-8、LFで記載願います。<br>\n",
    "\n",
    "\n",
    "\n",
    "・当ファイルを実行すると、「./output/」配下の全ファイルが削除されて新ファイルが生成されるのでご注意ください。<br>\n",
    "　必要ならば事前に退避しておいてください。<br>\n",
    "\n",
    "\n",
    "・実行時には、「./output/」配下のファイルを開かないでください。<br>\n",
    "\n",
    "・「Kernel」-「Restart & Clear Output」  →  「Kernel」-「Restart & Run All」を押下して実行願います。<br>\n",
    "　　初回実行時に、プロンプトにてファイル名の入力・第一カラム名の入力・性能評価指標の選択・アルゴリズムの選択を求められるのでご注意ください。<br>\n",
    "    \n",
    "     以下のメッセージが出力されれば終了です。\n",
    "     ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n",
    "    【祝】ファイル出力が完了しました(./output/feature_nnn_yyyyy/xxxxxx.csv)♪\n",
    "     ★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blue\n",
    "\n",
    "# sklearn関連\n",
    "from IPython.core.display import display, HTML\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import os.path\n",
    "import shutil\n",
    "from IPython.core.display import display\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "from time import sleep\n",
    "import configparser\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# アルゴリズム\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "# 性能指標\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 数値関連\n",
    "import math\n",
    "\n",
    "# パーサー\n",
    "from dateutil.parser import parse\n",
    "\n",
    "# XML\n",
    "import xml.etree.ElementTree as ET\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# 次元削減\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# HoldOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# k-fold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# 描画\n",
    "import matplotlib\n",
    "from matplotlib import pylab as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.ensemble.partial_dependence import plot_partial_dependence\n",
    "\n",
    "# パラメータ探索\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# warning無視\n",
    "import warnings\n",
    "\n",
    "# 乱数\n",
    "from numpy import *\n",
    "\n",
    "\n",
    "# 不均衡調整\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler,SMOTE\n",
    "\n",
    "# pkl\n",
    "import pickle\n",
    "\n",
    "#P↓ythonでプログラムを終了させるで必要：sys.exit()\n",
    "import sys\n",
    "\n",
    "# HyperOpt\n",
    "from hyperopt import fmin, tpe, hp, rand , space_eval\n",
    "\n",
    "\n",
    "# ↓コメントアウトを外すと、ブレークポイントが使えるようになる。\n",
    "#from IPython.core.debugger import Pdb; Pdb().set_trace()\n",
    "\n",
    "# うざいからwarningは無視\n",
    "#warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●セルの背景色設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# セルの背景色の設定\n",
    "# インポートなど：青色、関数:緑色、変数・定数宣言:黄色\n",
    "# 後ろのセルから色を変える。\n",
    "display(HTML(\"\"\"\n",
    "<script>\n",
    "let cells = $(\".cell\");\n",
    "for(let i=0;i<cells.length;i++){    \n",
    "    if( $(cells[i]).text().includes(\"#blue\") ){\n",
    "        $(cells[i]).find(\".CodeMirror\").css(\"background-color\",\"#94ffff\");\n",
    "    }\n",
    "    else if( $(cells[i]).text().includes(\"#green\") ){\n",
    "        $(cells[i]).find(\".CodeMirror\").css(\"background-color\",\"#efe\");\n",
    "    }\n",
    "    else if( $(cells[i]).text().includes(\"#yellow\") ){\n",
    "        $(cells[i]).find(\".CodeMirror\").css(\"background-color\",\"#ffff7c\");\n",
    "    }\n",
    "    else if( $(cells[i]).text().includes(\"#pink\") ){\n",
    "        $(cells[i]).find(\".CodeMirror\").css(\"background-color\",\"#ffc0cb\");\n",
    "    }\n",
    "    else if( $(cells[i]).text().includes(\"#gray\") ){\n",
    "        $(cells[i]).find(\".CodeMirror\").css(\"background-color\",\"#ededed\");\n",
    "    }\n",
    "    else{\n",
    "        $(cells[i]).find(\".CodeMirror\").css(\"background-color\",\"#f7f7f7\");\n",
    "    }\n",
    "}\n",
    "</script>\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●環境設定ファイル読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blue\n",
    "### 環境設定ファイル読み込み\n",
    "inifile = configparser.ConfigParser()\n",
    "inifile.read('./config.ini', 'UTF-8')\n",
    "conf_exe_mode =  inifile.get('settings', 'exe_mode')\n",
    "conf_pkl_del_mode =  inifile.get('settings', 'pkl_del_mode')\n",
    "conf_input_folder_path =  inifile.get('settings', 'input_folder_path')\n",
    "conf_output_folder_path =  inifile.get('settings', 'output_folder_path')\n",
    "conf_xml_folder_path =  inifile.get('settings', 'xml_folder_path')\n",
    "conf_pkl_folder_path =  inifile.get('settings', 'pkl_folder_path')\n",
    "conf_reha_train_input_file = inifile.get('settings', 'reha_train_input_file')\n",
    "conf_reha_test_input_file = inifile.get('settings', 'reha_test_input_file')\n",
    "conf_submission_file = inifile.get('settings', 'submission_file')\n",
    "conf_reha_one_hot_encode_column = inifile.get('settings', 'reha_one_hot_encode_column')\n",
    "conf_reha_datetime_column = inifile.get('settings', 'reha_datetime_column')\n",
    "conf_eval_method = int(inifile.get('settings', 'eval_method'))\n",
    "conf_cv_num = int(inifile.get('settings', 'cv_num'))\n",
    "conf_grid_cv_num = int(inifile.get('settings', 'grid_cv_num'))\n",
    "conf_feature_num_val = int(inifile.get('settings', 'feature_num'))\n",
    "conf_one_hot_encode_column_sitei = inifile.get('settings', 'one_hot_encode_column_sitei')\n",
    "conf_datetime_column_sitei = inifile.get('settings', 'datetime_column_sitei')\n",
    "conf_reha_first_column_name = inifile.get('settings', 'reha_first_column_name')\n",
    "conf_row_num_limit_val = int(inifile.get('settings', 'row_num_limit'))\n",
    "conf_unbalance_adj_mode = int(inifile.get('settings', 'unbalance_adj_mode'))\n",
    "conf_col_proba = inifile.get('settings', 'col_proba')\n",
    "\n",
    "\n",
    "print(\"conf_unbalance_adj_mode =\",conf_unbalance_adj_mode)\n",
    "print(\"conf_exe_mode =\",conf_exe_mode)\n",
    "print(\"conf_input_folder_path =\",conf_input_folder_path)\n",
    "print(\"conf_output_folder_path =\",conf_output_folder_path)\n",
    "print(\"conf_xml_folder_path =\",conf_xml_folder_path)\n",
    "print(\"conf_pkl_folder_path =\",conf_pkl_folder_path)\n",
    "print(\"conf_eval_method =\",conf_eval_method)\n",
    "print(\"conf_grid_cv_num =\",conf_grid_cv_num)\n",
    "print(\"conf_feature_num_val =\",conf_feature_num_val) \n",
    "print(\"conf_row_num_limit_val =\",conf_row_num_limit_val) \n",
    "print(\"conf_col_proba =\",conf_col_proba) \n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"+++++ ↓はリハーサル時のみ使われる↓ +++++\")\n",
    "print(\"conf_reha_train_input_file =\",conf_reha_train_input_file)\n",
    "print(\"conf_reha_test_input_file =\",conf_reha_test_input_file)\n",
    "print(\"conf_submission_file =\",conf_submission_file)\n",
    "print(\"conf_reha_one_hot_encode_column =\",conf_reha_one_hot_encode_column) \n",
    "print(\"conf_reha_datetime_column =\",conf_reha_datetime_column) \n",
    "print(\"conf_reha_first_column_name =\",conf_reha_first_column_name) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●outputフォルダ配下の全ファイル削除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blue\n",
    "#outputフォルダ配下の全ファイル削除\n",
    "output_file_list=os.listdir(conf_output_folder_path)\n",
    "print(output_file_list)\n",
    "for ouput_file in output_file_list:\n",
    "    if(os.path.isfile(conf_output_folder_path + ouput_file)):\n",
    "        # ファイル削除\n",
    "        os.remove(conf_output_folder_path + ouput_file)\n",
    "    else:\n",
    "        # フォルダ削除\n",
    "        shutil.rmtree(conf_output_folder_path + ouput_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●変数・定数の宣言"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#yellow\n",
    "\n",
    "\n",
    "#### 定数定義\n",
    "# 実行モード:本番\n",
    "EXE_MODE_PROD = \"0\"\n",
    "# 実行モード:リハーサル\n",
    "EXE_MODE_REHA = \"1\"\n",
    "# 欠損埋めの数値\n",
    "FILL_NAN_VALUE = 0\n",
    "# DataFrameを表示する行数\n",
    "PRINT_DF_LINENUM = 5\n",
    "# 訓練用データ\n",
    "FILE_TYPE_NAME_TRAIN = \"訓練用データ\"\n",
    "# 検証用データ\n",
    "FILE_TYPE_NAME_TEST = \"検証用データ\"\n",
    "# ハイパーパラメータXML\n",
    "FILE_TYPE_HYPER_PARA_NAME = \"ハイパーパラメータXML\"\n",
    "\n",
    "#### 事前評価の方法\n",
    "# HOLTOUT\n",
    "PRE_PREDICT_HOLDOUT = \"holdout\"\n",
    "# K_FOLD\n",
    "PRE_PREDICT_K_FOLD = \"k-fold\"\n",
    "\n",
    "\n",
    "#  TODO cv_num 事前検証用の値\n",
    "CV_NUM_DEFAULT = 3\n",
    "#CV_NUM_DEFAULT = 2\n",
    "\n",
    "# 指定なし\n",
    "SITEI_NASI = \"0\"\n",
    "# 指定あり\n",
    "SITEI_ARI = \"1\"\n",
    "\n",
    "# 時刻型\n",
    "DATE_TYPE = \"datetime64[ns]\"\n",
    "\n",
    "\n",
    "# 型名\n",
    "TYPE_INT=\"int\"\n",
    "TYPE_FLOAT=\"float\"\n",
    "TYPE_DOUBLE=\"double\"\n",
    "TYPE_BOOL=\"bool\"\n",
    "TYPE_STR=\"string\"\n",
    "\n",
    "\n",
    "# 経過時間変換の開始時刻\n",
    "START_TIME = '1970-1-1 00:00:00'\n",
    "# random_stateの値\n",
    "RANDOM_STATE_VAL = 1\n",
    "# test_size\n",
    "#TEST_SIZE_VAL = 0.20\n",
    "TEST_SIZE_VAL = 0.3\n",
    "\n",
    "# 特徴量削減ステップ\n",
    "SELECTOR_STEP = .05\n",
    "\n",
    "# 評価方法の番号\n",
    "K_FOLD_NUMBER = 1\n",
    "H_OUT_NUMBER = 2\n",
    "\n",
    "# 性能評価指標選択の番号\n",
    "ACCURACY_NUMBER = 1\n",
    "PRECISION_NUMBER = 2\n",
    "RECALL_NUMBER = 3\n",
    "F1_NUMBER = 4\n",
    "AUC_NUMBER = 5\n",
    "GO_NEXT = 99\n",
    "\n",
    "# 不均衡調整モード\n",
    "UNB_ADJ_NASI = 0\n",
    "UNB_ADJ_UNDER = 1\n",
    "UNB_ADJ_OVER = 2\n",
    "UNB_ADJ_SMOTE = 3\n",
    "\n",
    "\n",
    "# アルゴリズム名\n",
    "ALG_NAME_LOGISTIC = \"ロジスティック回帰\"\n",
    "ALG_NAME_RANDOMFOREST = \"ランダムフォレスト\"\n",
    "ALG_NAME_MLP = \"多層パーセプトロン\"\n",
    "ALG_NAME_XGB = \"XGBoost\"\n",
    "\n",
    "# 性能評価指標・アルゴリズム選択の番号\n",
    "STOP_NUMBER = 100\n",
    "\n",
    "### 予測種別\n",
    "# クラスのラベル\n",
    "PREDICTTYPE_LABEL = 0\n",
    "# 予測確率\n",
    "PREDICTTYPE_PROBA = 1\n",
    "# 予測確率対象のクラス分類の値\n",
    "TARGET_PROB_CLASS_VALUE = 1\n",
    "# クラス変数のカラムINDEXの値\n",
    "CLASS_COL_INDEX = 1\n",
    "\n",
    "# 描画のフォントサイズ\n",
    "PLT_TITLE_FONT_SIZE=20\n",
    "PLT_LABEL_FONT_SIZE=14\n",
    "\n",
    "#### oneHotEncoder\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "\n",
    "#### 性能評価選択のディクショナリ\n",
    "# AUCかf1が望ましい。\n",
    "perfDict = {GO_NEXT:\"モデル選択へ進む\",ACCURACY_NUMBER:\"accuracy_score(正解率)\", PRECISION_NUMBER:\"precision_score(適合率)\", \\\n",
    "    RECALL_NUMBER:\"recall_score(再現率)\", F1_NUMBER:\"f1_score(F値)\", AUC_NUMBER:\"AUC\", STOP_NUMBER:\"中止\"}\n",
    "\n",
    "#### 性能評価ラベルのディクショナリ\n",
    "perfLabelDict = {ACCURACY_NUMBER:\"accuracy\", PRECISION_NUMBER:\"precision\", \\\n",
    "    RECALL_NUMBER:\"recall\", F1_NUMBER:\"f1\", AUC_NUMBER:\"roc_auc\"}\n",
    "\n",
    "\n",
    "#### モデル選択(評価方法)のディクショナリ\n",
    "evalDict = {K_FOLD_NUMBER:PRE_PREDICT_K_FOLD, H_OUT_NUMBER:PRE_PREDICT_HOLDOUT}\n",
    "\n",
    "# 特徴量上限数\n",
    "feature_num_lim_val = conf_feature_num_val\n",
    "\n",
    "#### 第一カラム名\n",
    "g_first_column_name = None\n",
    "\n",
    "#### モデルスプリット用\n",
    "MODEL_SPRIT = \"  \"\n",
    "\n",
    "# FLAG\n",
    "FLAG_OFF = \"0\"\n",
    "FLAG_ON = \"1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●関数の宣言(軽微なやつ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#green\n",
    "\n",
    "####################################\n",
    "##### ↓ファイル名取得関数↓\n",
    "####################################\n",
    "def getInputFileName(fileTypeName, backStr):\n",
    "    #### ↓ループ↓\n",
    "    while True:\n",
    "        # ファイル名をプロンプトから入力\n",
    "        input_file_name = getInputStr(fileTypeName + 'のファイル名を入力してください。' + backStr)\n",
    "        \n",
    "        if(fileTypeName == FILE_TYPE_NAME_TRAIN or fileTypeName == FILE_TYPE_NAME_TEST):\n",
    "            directoryPath = conf_input_folder_path\n",
    "        elif(fileTypeName == FILE_TYPE_HYPER_PARA_NAME):\n",
    "            directoryPath = conf_xml_folder_path\n",
    "        else:\n",
    "            # 異常なファイルタイプ(ありえないルート)\n",
    "            raise RuntimeError (\"【ERROR】異常なファイルタイプです。実行を中止しました。！\")\n",
    "\n",
    "        if(os.path.isfile(directoryPath +'%s' % input_file_name)):\n",
    "            return input_file_name\n",
    "            #終了\n",
    "        else:\n",
    "            print(\"【ERROR】ファイルが存在しません！正しく入力してください。\")\n",
    "            #ループ継続\n",
    "    #### ↑ループ↑\n",
    "\n",
    "\n",
    "####################################\n",
    "##### ↓文字列入力関数↓\n",
    "####################################\n",
    "def getInputStr(paramStr):\n",
    "    \n",
    "    #### ↓ループ↓\n",
    "    while True:\n",
    "        ## >>> 入力プロンプト登場\n",
    "        input_str = input(paramStr + '\\r\\n\\r\\n>>>  ')\n",
    "        #入力文字列が空か？\n",
    "        if(input_str == None or input_str == \"\"):\n",
    "            print(\"【ERROR】入力文字列が空です！\")\n",
    "            #ループ継続\n",
    "        else:\n",
    "            # 入力文字列返却\n",
    "            return input_str\n",
    "    #### ↑ループ↑\n",
    "\n",
    "####################################\n",
    "##### ↓DataFrame表示関数\n",
    "####################################\n",
    "def displayPrintDf(inputDf,printFormat,paramStr,line_num):\n",
    "    if line_num == None:\n",
    "        if paramStr == None or paramStr == \"\":\n",
    "            print(printFormat)\n",
    "        else:\n",
    "            print(printFormat % paramStr)\n",
    "        display(inputDf.head())\n",
    "    else:\n",
    "        if paramStr == None or paramStr == \"\":\n",
    "            print(printFormat % (line_num))\n",
    "        else:\n",
    "            print(printFormat % (paramStr,line_num))\n",
    "        display(inputDf.head(line_num))\n",
    "    print(\"(行数 , 列数) = \", inputDf.shape)\n",
    "    print(\"\\r\\n\")\n",
    "\n",
    "    \n",
    "####################################\n",
    "##### 【未使用】\n",
    "##### 時刻カラム読み出し時に適用するパーサ\n",
    "####################################\n",
    "def myparser (dateTimeStr):\n",
    "    # 欠損の場合\n",
    "    if(dateTimeStr == 'nan'):\n",
    "        # そのまま返す。\n",
    "        # 後で呼ばれる欠損埋め処理にて平均値を入れる。\n",
    "        return dateTimeStr\n",
    "\n",
    "    # 「/」 → 「-」 に変換\n",
    "    dateTimeStr = dateTimeStr.replace('/','-')\n",
    "    hyphenCnt = dateTimeStr.count('-')\n",
    "    colonCnt = dateTimeStr.count(':')\n",
    "    # YYYY-M-D H:M ～  YYYY-MM-DD HH:MM:SS の場合\n",
    "    if((hyphenCnt == 2 and (colonCnt == 1 or colonCnt == 2) ) \\\n",
    "        and \\\n",
    "        (12 <= len(dateTimeStr) and len(dateTimeStr) <= 19) ):\n",
    "        #「:」の出現回数をカウント\n",
    "        if(colonCnt == 2):\n",
    "            # YYYY-mm-dd HH:MM:SS 形式の場合\n",
    "            retDateTime = pd.datetime.strptime(dateTimeStr, '%Y-%m-%d %H:%M:%S')\n",
    "        else:\n",
    "            # YYYY-mm-dd HH:MM 形式の場合\n",
    "            retDateTime = pd.datetime.strptime(dateTimeStr, '%Y-%m-%d %H:%M')\n",
    "    else:\n",
    "        # 他の 形式の場合\n",
    "        # デフォルトのパーサー\n",
    "        retDateTime = parse(dateTimeStr)\n",
    "\n",
    "    return retDateTime\n",
    "\n",
    "####################################\n",
    "##### ↓DataFrameの行削除(暫定対処)↓\n",
    "####################################\n",
    "def delDfRow(fileTypeName, inputDf):\n",
    "    retDf = inputDf\n",
    "    ########\n",
    "    if(fileTypeName == FILE_TYPE_NAME_TRAIN\n",
    "       and\n",
    "       len(inputDf) > conf_row_num_limit_val):\n",
    "        \n",
    "        # 訓練データの行数が上限を超えている場合\n",
    "        raise ValueError (\"【ERROR】訓練用データの行数が上限を超えています。確認してください。\")\n",
    "        \n",
    "#         print(\"++++++++ 行数がconfigの上限を超過している！！ランダムに行を削除する。 ++++++++\")\n",
    "#         rowNumList = list()\n",
    "#         # リストが行数の上限に達するまでループ\n",
    "#         for i in range(0, len(inputDf)):\n",
    "#             # 「0 ～ 行数の最大-1」のリストを生成\n",
    "#             rowNumList.append(i)\n",
    "\n",
    "#         # リストの中からランダムに選択\n",
    "#         choiceList = random.choice(rowNumList, conf_row_num_limit_val, replace=False) \n",
    "        \n",
    "#         # 選ばれたindexの行だけDataFrameを抽出\n",
    "#         retDf = retDf.iloc[choiceList,:]\n",
    "        \n",
    "#         # index振り直し\n",
    "#         retDf = retDf.reset_index(drop=True)\n",
    "\n",
    "#         displayPrintDf(retDf, \"行数削除直後 %s の先頭%d行：\" ,fileTypeName,PRINT_DF_LINENUM)\n",
    "#         print(retDf.dtypes)\n",
    "    ########\n",
    "\n",
    "    return retDf\n",
    "\n",
    "\n",
    "####################################\n",
    "##### ↓入力ファイルからDataFrame取得↓\n",
    "####################################\n",
    "def getInputDf(fileTypeName, exe_mode, fileName,  oheList, dateList, date_sitei):\n",
    "    global g_first_column_name\n",
    "    target_obj_dict = {}\n",
    "    \n",
    "    # 第一カラム名取得。初回だけ実施\n",
    "    if(g_first_column_name == None):\n",
    "        if(exe_mode == EXE_MODE_PROD):\n",
    "            # 本番の場合 プロンプトから入力\n",
    "            g_first_column_name = getInputStr('第一カラム名を入力してください')\n",
    "        else:\n",
    "            # リハーサルの場合 configから入力\n",
    "            g_first_column_name = conf_reha_first_column_name\n",
    "        \n",
    "    target_obj_dict[g_first_column_name] = object\n",
    "    for columnName in oheList:\n",
    "        target_obj_dict[columnName] = object\n",
    "            \n",
    "    # OneHotEncoder対象をobject指定で読み込み 時刻指定カラムをパーサーかませて読み込み\n",
    "    if(date_sitei == SITEI_ARI):\n",
    "        # 時刻カラム指定あり\n",
    "        inputDf = pd.read_csv(conf_input_folder_path +'%s' % fileName,\\\n",
    "            dtype=target_obj_dict ,parse_dates=dateList, date_parser=myparser)\n",
    "    else:\n",
    "        # 時刻カラム指定なし\n",
    "        inputDf = pd.read_csv(conf_input_folder_path +'%s' % fileName,\\\n",
    "            dtype=target_obj_dict)\n",
    "        \n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    # TODO\n",
    "    # read_csv ハイパーパラメータの例\n",
    "        # encoding=‘utf-8' or 'utf_8'  デフォルト、'shift_jis'\n",
    "        # lineterminator : string, default None\n",
    "        #           改行コード：  '\\r'(デフォルト) or '\\r\\n' or '\\n'\n",
    "        # header=0  int (ヘッダの行番号:0始まり。デフォルトで0。)\n",
    "        # quotechar クォテーションを指定するパラメータ。デフォルトは”（ダブルクォテーション）。\n",
    "        #           '\\'' or '\"'\n",
    "    # ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    \n",
    "    # 検証用の場合\n",
    "    if(fileTypeName == FILE_TYPE_NAME_TEST):\n",
    "        # ID(第1カラム)がNANの行は「未指定」に置換する。\n",
    "        inputDf = inputDf.fillna({inputDf.columns.values[0]: '未指定'})\n",
    "\n",
    "    \n",
    "    # 出力\n",
    "    displayPrintDf(inputDf, \"csv読み込み直後 %s の先頭%d行：\" ,fileTypeName,PRINT_DF_LINENUM)\n",
    "    print(inputDf.dtypes)\n",
    "    \n",
    "    # DataFrameの行数が、configの上限を超えている場合の処理を行う。\n",
    "    inputDf = delDfRow(fileTypeName, inputDf)\n",
    "    \n",
    "    return inputDf\n",
    "\n",
    "\n",
    "####################################\n",
    "#### csvファイルを読み込みんでDfを返す。\n",
    "####################################\n",
    "def getDfByReadCSV(fileTypeName, exe_mode, input_reha_file_name , input_ohe_clm_list, \\\n",
    "    input_datetime_clm_list, date_sitei):\n",
    "    if(exe_mode == EXE_MODE_PROD):\n",
    "        retDf = getInputDf(fileTypeName, exe_mode, getInputFileName(fileTypeName, \"\"), \\\n",
    "                           input_ohe_clm_list, input_datetime_clm_list,\\\n",
    "                           date_sitei)\n",
    "    else:\n",
    "        retDf = getInputDf(fileTypeName, exe_mode, input_reha_file_name,\n",
    "                           input_ohe_clm_list, input_datetime_clm_list,\\\n",
    "                           date_sitei)\n",
    "        \n",
    "    return retDf\n",
    "\n",
    "####################################\n",
    "##### ↓One Hot Encoder関数(数値でカテゴリを指定された場合も動く)\n",
    "####################################\n",
    "def oneHotEncoderExe(fileTypeName, i_ohe_clm_list, inputDf, column_sitei):\n",
    "    \n",
    "    displayPrintDf(inputDf,\"%s One Hot Encoder差し替え前の先頭%d行：\" ,fileTypeName,PRINT_DF_LINENUM)\n",
    "\n",
    "    # OneHotEncoding実施\n",
    "    # NaNも一つのカテゴリーとしてダミー変数化したい場合は、引数dummy_na=Trueとする。\n",
    "    if(column_sitei == SITEI_ARI):\n",
    "        print(\"OneHotEncoding カラム指定あり\")\n",
    "        oheDf = pd.get_dummies(inputDf,\\\n",
    "            dummy_na=True,\n",
    "            columns=i_ohe_clm_list)\n",
    "    else:\n",
    "        print(\"OneHotEncoding カラム指定なし\")\n",
    "        oheDf = pd.get_dummies(inputDf,\\\n",
    "            dummy_na=True)\n",
    "\n",
    "    displayPrintDf(oheDf,\"%s One Hot Encoder差し替え後の先頭%d行：\" ,fileTypeName,PRINT_DF_LINENUM)\n",
    "\n",
    "    return oheDf\n",
    "\n",
    "####################################\n",
    "##### ↓検証用データのカラム合わせ込み関数\n",
    "####################################\n",
    "def adjustTestDfColumn(inputTrainDf, inputTestDf):\n",
    "    displayPrintDf(inputTrainDf,\\\n",
    "        \"++++++ ↓%sの先頭%d行(カラム合わせ込み前)↓ ++++++\",FILE_TYPE_NAME_TRAIN,PRINT_DF_LINENUM)\n",
    "    displayPrintDf(inputTestDf,\\\n",
    "        \"++++++ ↓%sの先頭%d行(カラム合わせ込み前)↓ ++++++\",FILE_TYPE_NAME_TEST,PRINT_DF_LINENUM)\n",
    "\n",
    "    trainColumnList = inputTrainDf.columns.values\n",
    "    cols_train = set(trainColumnList)\n",
    "    cols_test = set(inputTestDf.columns.values)\n",
    "\n",
    "    # 訓練用にはあったが検証用にはないデータ項目\n",
    "    diff1 = cols_train - cols_test\n",
    "    #print('訓練用のみに存在する項目: %s' % diff1)\n",
    "    # 検証用にはあるが訓練用になかったデータ項目\n",
    "    diff2 = cols_test - cols_train\n",
    "    #print('検証用のみに存在する項目: %s' % diff2)\n",
    "\n",
    "    print('== 検証用のみに存在する項目を削除し、訓練用にのみ存在する項目を追加して初期値で埋める ==')\n",
    "    # 検証用のみに存在する項目を削除\n",
    "    retTestDf = inputTestDf.drop(list(diff2),\\\n",
    "        axis=1)\n",
    "\n",
    "    # 訓練用データの空のDataFrameを作る。\n",
    "    df_cols_train = pd.DataFrame(None,\\\n",
    "                             columns=trainColumnList,\\\n",
    "                             dtype=float)\n",
    "    # df_cols_trainを、検証用データに結合(データがあるやつはそのまま)\n",
    "    # 列名で紐づけて連結される\n",
    "    retTestDf = pd.concat([df_cols_train, retTestDf])\n",
    "    # さらにNANを初期値で埋める。\n",
    "    retTestDf = retTestDf.fillna(FILL_NAN_VALUE, axis=1)\n",
    "\n",
    "    cols_test = set(retTestDf.columns.values)\n",
    "    # 訓練用にはあったが検証用にはないデータ項目\n",
    "    diff1 = cols_train - cols_test\n",
    "    # 検証用にはあるが訓練用になかったデータ項目\n",
    "    diff2 = cols_test - cols_train\n",
    "    # いずれかに差分があった場合\n",
    "    if(len(diff1) > 0 or len(diff2) > 0):\n",
    "        # 絶対にここに来ることはない。来たら、これより前の処理でバグがあることを意味する！！！\n",
    "        raise ValueError (\"【ERROR】カラム合わせこみの結果が不正です。\")\n",
    "\n",
    "\n",
    "    # 訓練用データのデータ項目の並び順でソートする。\n",
    "    retTestDf = retTestDf.reindex(trainColumnList, axis=1)\n",
    "\n",
    "    # この時点での検証用データ表示\n",
    "    displayPrintDf(retTestDf,\\\n",
    "        \"++++++ ↓%sの先頭%d行(カラム合わせ込み後)↓ ++++++\",FILE_TYPE_NAME_TEST,PRINT_DF_LINENUM)\n",
    "    return retTestDf\n",
    "\n",
    "####################################\n",
    "##### ↓ファイル出力\n",
    "####################################\n",
    "def fileOutput(inputDf, dirName, outFileName, lineCnt, finalFlag, i_feature_dir):\n",
    "    # コンソール出力\n",
    "    # 提出用CSV出力\n",
    "    displayPrintDf(inputDf,\\\n",
    "        \"++++++ ↓出力対象Dfの先頭%d行↓ ++++++\", None, lineCnt)\n",
    "\n",
    "    fileDirPath = conf_output_folder_path + i_feature_dir\n",
    "    \n",
    "    # 特徴量ディレクトリ\n",
    "    if(os.path.exists(fileDirPath) != True):\n",
    "        # ディレクトリが存在しない場合\n",
    "        # ディレクトリ作成\n",
    "        os.mkdir(fileDirPath)\n",
    "\n",
    "    if(dirName != \"\"):\n",
    "        # ディレクトリ名指定有りの場合\n",
    "        if(os.path.exists(fileDirPath + dirName) != True):\n",
    "            # ディレクトリが存在しない場合\n",
    "            # ディレクトリ作成\n",
    "            os.mkdir(fileDirPath + dirName)\n",
    "        dirName = dirName + \"/\"\n",
    "\n",
    "#     # 「SYLKファイルであることを確認しましたが、読み込むことができません。」\n",
    "#     # を回避するため列名「ID」を\"で囲む。「ID」がなければ何も変わらない。\n",
    "#     inputDf = inputDf.rename(columns={'ID': '\"ID\"'})\n",
    "\n",
    "    # ファイル出力\n",
    "    inputDf.to_csv(fileDirPath + dirName + outFileName ,index=False, encoding=\"shift_jis\")\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    if finalFlag == True:\n",
    "        print(\"★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\")\n",
    "        print(\"【祝】ファイル出力が完了しました(%s)♪\" % (fileDirPath + dirName + outFileName))\n",
    "        print(\"★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\")\n",
    "    else:\n",
    "        print(\"ファイル出力をしました(%s)。\" % (fileDirPath + dirName + outFileName))\n",
    "\n",
    "####################################\n",
    "##### ↓プロンプト入力関数(入力番号を返す)\n",
    "####################################\n",
    "def getInputNumber(selTypeName, afterStr):\n",
    "    # 1秒待つ\n",
    "    sleep(1)\n",
    "    #### ↓ループ↓\n",
    "    while True:\n",
    "        inputPerStr = selTypeName + \"を入力してください。\" + afterStr\n",
    "        ## >>> 入力プロンプト登場\n",
    "        input_item = input(inputPerStr + '\\n\\n>>>  ')\n",
    "        #選択した評価指標が空か？\n",
    "        if(input_item == None or input_item == \"\"):\n",
    "            return None\n",
    "        else:\n",
    "            #入力値あり\n",
    "            try:\n",
    "                #int 型に変換\n",
    "                return int(input_item)\n",
    "            except:\n",
    "                print(\"入力値が不正です。再入力してください。\\r\\n\" , e)\n",
    "                sleep(1)\n",
    "    #### ↑ループ↑\n",
    "        \n",
    "####################################\n",
    "##### ↓プロンプト入力関数(入力番号を返す(ディクショナリ))\n",
    "####################################\n",
    "def getInputNumberDic(selTypeName,inputDict):\n",
    "    # 1秒待つ\n",
    "    sleep(1)\n",
    "    #### ↓ループ↓\n",
    "    while True:\n",
    "        inputPerStr = selTypeName + \"を選択してください。\" + str(inputDict)\n",
    "        ## >>> 入力プロンプト登場\n",
    "        input_item = input(inputPerStr + '\\n\\n>>>  ')\n",
    "        #選択した評価指標が空か？\n",
    "        if(input_item == None or input_item == \"\"):\n",
    "            print(\"【ERROR】入力してください！\")\n",
    "            #ループ継続\n",
    "        else:\n",
    "            #int 型に変換\n",
    "            input_item_num = int(input_item)\n",
    "            \n",
    "            value_item = inputDict.get(input_item_num)\n",
    "            if(value_item == None or value_item == \"\"):\n",
    "                print(\"【ERROR】範囲内の値を入力してください！\")\n",
    "                #ループ継続\n",
    "            else:\n",
    "                print(\"入力されたのは [%s:%s] です\" % (input_item_num,value_item))\n",
    "                return input_item_num\n",
    "    #### ↑ループ↑\n",
    "\n",
    "####################################\n",
    "##### ↓外部ファイルを読み込んで値をリストで返却する関数\n",
    "####################################\n",
    "def getFileValueList(sitei_mode, filePath, fileName):\n",
    "    value_list = list()\n",
    "    \n",
    "    if(sitei_mode == SITEI_ARI):\n",
    "        # 指定ありの場合 ファイルを読み込んでリストに詰める\n",
    "        value_data = open(filePath + fileName, \"r\", encoding=\"utf-8\")\n",
    "        # TODO 文字コード指定\n",
    "        # Shift-JIS の場合は、encoding=\"shift_jis\"\n",
    "        for value_data_line in value_data:\n",
    "            value_data_line = value_data_line.replace('\\n','').replace('\\r','')\n",
    "            value_list.append(value_data_line)\n",
    "            \n",
    "\n",
    "    print(\"★\" + fileName + \"から読み込んだカラム=\",value_list)\n",
    "    sleep(1)\n",
    "    return value_list\n",
    "\n",
    "####################################\n",
    "##### ↓設定値決定\n",
    "####################################\n",
    "def getSettingValue(confSettingValue,prodSettingValue,exe_mode):\n",
    "    if(exe_mode == EXE_MODE_PROD):\n",
    "        retSettingValue = prodSettingValue\n",
    "    else:\n",
    "        retSettingValue = confSettingValue \n",
    "        \n",
    "    return retSettingValue\n",
    "\n",
    "####################################\n",
    "##### 【未使用】\n",
    "##### 経過時間の計算\n",
    "##### ↓日付時分秒のデータを経過時間に置き換える。\n",
    "####################################\n",
    "def getConvTimeDf(fileTypeName, inputDf):\n",
    "    # 変換前\n",
    "    displayPrintDf(inputDf,fileTypeName + \" 時刻変換前inputDfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "\n",
    "#     print(\"スケールが大きいので対数を取る\") \n",
    "#     dateTimeDf['Outlet_Establishment_Year'] = math.log(dateTimeDf['Outlet_Establishment_Year'])\n",
    "#     displayPrintDf(dateTimeDf,fileTypeName + \" dateTimeDfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "\n",
    "    column_list = inputDf.columns\n",
    "    for columnName in column_list:\n",
    "        # 時刻型の場合\n",
    "        if(inputDf[columnName].dtype == DATE_TYPE):\n",
    "            print(\"時刻型のカラム：\",columnName)\n",
    "            dateTimeDf = inputDf[columnName]\n",
    "            # UNIX開始時刻からの経過時間に変換する。\n",
    "            timediff = dateTimeDf - pd.to_datetime(START_TIME,format='%Y%m%d %H:%M:%S')\n",
    "            dateTimeDf = timediff/np.timedelta64(1, 's')\n",
    "            #変換した値に置換する\n",
    "            inputDf[columnName] = dateTimeDf\n",
    "    \n",
    "    # 変換後\n",
    "    displayPrintDf(inputDf,fileTypeName + \" 時刻変換後inputDfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "\n",
    "    return inputDf\n",
    "\n",
    "####################################\n",
    "##### 次元削減(特徴量選択) を使用する\n",
    "##### ↓特徴量選択(RandomForestClassifier)\n",
    "####################################\n",
    "def getSelectorFeature(fileTypeName,inputFeatureDf,y_Df,feature_num):\n",
    "    # 特徴量選択前\n",
    "    displayPrintDf(inputFeatureDf,fileTypeName + \" 特徴量選択前Dfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "\n",
    "    # 特徴量因子の重要度を推定する分類器をRandomForestClassifierに設定\n",
    "    # 最終的に残す特徴量をconfig.iniに定義している\n",
    "    # 0未満 の場合はNoneに設定する。\n",
    "    # 1回のstepで削除する次元数は5%ずつとする\n",
    "    \n",
    "    print(\"特徴量選択数 = \",feature_num)\n",
    "    if(feature_num < 0):\n",
    "        # デフォルト値\n",
    "        print(\"n_features_to_select = None に設定\")\n",
    "        feature_num = None\n",
    "    \n",
    "    # 特徴量選択\n",
    "    # step:特徴量削除の速度。一度の再帰処理により指定ステップ分の特徴量が消滅する。\n",
    "    selector = RFE(estimator=RandomForestClassifier(random_state=RANDOM_STATE_VAL),\n",
    "                   n_features_to_select=feature_num,\n",
    "                   step=SELECTOR_STEP)\n",
    "    selector.fit(inputFeatureDf,y_Df.values.ravel())\n",
    "\n",
    "    # 削除対象配列(Falseが削除対象)\n",
    "    selFlagArray = selector.support_\n",
    "    \n",
    "    # 現状のカラム名を取得\n",
    "    columnIndex = inputFeatureDf.columns\n",
    "    \n",
    "    # 削除対象カラム名リスト\n",
    "    drop_column_list = list()\n",
    "    # 削除対象配列 でループ(i=0,1,2,・・・)\n",
    "    print(len(selFlagArray))\n",
    "    for i in range(len(selFlagArray)):\n",
    "        # Falseの場合に、削除対象リストに追加\n",
    "        if(selFlagArray[i] == False):\n",
    "            drop_column_list.append(columnIndex[i])\n",
    "\n",
    "    # 不要な列削除\n",
    "    #print(\"削除対象カラム = \" , drop_column_list)\n",
    "    inputFeatureNewDf = inputFeatureDf.drop(drop_column_list, axis=1)\n",
    "    \n",
    "    # 特徴量選択後\n",
    "    displayPrintDf(inputFeatureNewDf,fileTypeName + \" 特徴量選択後Dfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "\n",
    "    return inputFeatureNewDf\n",
    "\n",
    "\n",
    "####################################\n",
    "##### ↓TRIM関数↓\n",
    "####################################\n",
    "def strTrim(inputStr):\n",
    "    trimStr = inputStr.strip()\n",
    "    return trimStr\n",
    "\n",
    "\n",
    "####################################\n",
    "##### ↓XML読み込み↓\n",
    "####################################\n",
    "def xmlRead(fileName):\n",
    "    hyperOptFlag = False\n",
    "    xmlDict = dict()\n",
    "    # paramsタグ\n",
    "    # xmlファイルの読み込み\n",
    "    tree = ET.parse(conf_xml_folder_path + fileName)\n",
    "    root = tree.getroot()\n",
    "    for child1 in root:\n",
    "        c1Text = strTrim(child1.text)\n",
    "        if(c1Text != None and c1Text != \"\"):\n",
    "            # algorithm\n",
    "            if(child1.tag in xmlDict):\n",
    "                raise ValueError (\"【ERROR】タグが重複しています：\" + child1.tag)\n",
    "            algoStr = c1Text\n",
    "        for child2 in child1:\n",
    "\n",
    "            # パラメータ名\n",
    "            if(child2.tag in xmlDict):\n",
    "                raise ValueError (\"【ERROR】タグが重複しています：\" + child2.tag)\n",
    "\n",
    "            xmlDict[child2.tag] = []\n",
    "            rangeMin = 0\n",
    "            rangeMax = 0\n",
    "            rangeIncre = 0\n",
    "            hpChoiceArray = []\n",
    "            lowVal = None\n",
    "\n",
    "            for child3 in child2:\n",
    "                # 型、値\n",
    "                paramArray = xmlDict[child2.tag]\n",
    "                c3Text = strTrim(child3.text)\n",
    "                if(c3Text == None or c3Text == \"\"):\n",
    "                    raise ValueError (\"【ERROR】値が空です：\" + child3.tag)\n",
    "                else:\n",
    "                    if(child3.tag == \"type\"):\n",
    "                        valueType = c3Text\n",
    "                    ############################################################################\n",
    "                    # ここからはGridSearchCV用。\n",
    "                    ############################################################################\n",
    "                    elif(re.match('value*', child3.tag)):\n",
    "                        if(valueType == TYPE_INT):\n",
    "                            # int型の場合\n",
    "                            paramVal = int(c3Text)\n",
    "                        elif(valueType == TYPE_FLOAT):\n",
    "                            # float型の場合\n",
    "                            paramVal = float(c3Text)\n",
    "                        elif(valueType == TYPE_DOUBLE):\n",
    "                            # double型の場合\n",
    "                            paramVal = double(c3Text)\n",
    "                        elif(valueType == TYPE_BOOL):\n",
    "                            # bool型の場合\n",
    "                            # まずは文字列で読みだしてからboolに変換する。\n",
    "                            paramVal = str(c3Text)\n",
    "                            if(paramVal == \"True\"):\n",
    "                                paramVal = True\n",
    "                            elif(paramVal == \"False\"):\n",
    "                                paramVal = False\n",
    "                            else:\n",
    "                                raise ValueError (\"【ERROR】値が不正です：\" + child3.tag)\n",
    "                        else:\n",
    "                            # string型の場合\n",
    "                            paramVal = str(c3Text)\n",
    "\n",
    "                        # 設定\n",
    "                        paramArray.append(paramVal)\n",
    "                        xmlDict[child2.tag] = paramArray\n",
    "                    elif(re.match('rangeMin', child3.tag)):\n",
    "                        if(valueType == TYPE_INT):\n",
    "                            # int型の場合\n",
    "                            rangeMin = int(c3Text)\n",
    "                        elif(valueType == TYPE_FLOAT):\n",
    "                            # float型の場合\n",
    "                            rangeMin = float(c3Text)\n",
    "                        elif(valueType == TYPE_DOUBLE):\n",
    "                            # double型の場合\n",
    "                            rangeMin = double(c3Text)\n",
    "                    elif(re.match('rangeMax', child3.tag)):\n",
    "                        if(valueType == TYPE_INT):\n",
    "                            # int型の場合\n",
    "                            rangeMax = int(c3Text)\n",
    "                        elif(valueType == TYPE_FLOAT):\n",
    "                            # float型の場合\n",
    "                            rangeMax = float(c3Text)\n",
    "                        elif(valueType == TYPE_DOUBLE):\n",
    "                            # double型の場合\n",
    "                            rangeMax = double(c3Text)\n",
    "                    elif(re.match('rangeIncre', child3.tag)):\n",
    "                        if(valueType == TYPE_INT):\n",
    "                            # int型の場合\n",
    "                            rangeIncre = int(c3Text)\n",
    "                        elif(valueType == TYPE_FLOAT):\n",
    "                            # float型の場合\n",
    "                            rangeIncre = float(c3Text)\n",
    "                        elif(valueType == TYPE_DOUBLE):\n",
    "                            # double型の場合\n",
    "                            rangeIncre = double(c3Text)\n",
    "                        # xmlDictに設定\n",
    "                        xmlDict[child2.tag] = range(rangeMin, rangeMax, rangeIncre)\n",
    "                    ############################################################################\n",
    "                    # ここからはHyperOpt用。\n",
    "                    ############################################################################\n",
    "                    elif(re.match('hp.*', child3.tag)):\n",
    "                        # HyperOpt\n",
    "                        hyperOptFlag = True\n",
    "                        xmlDict, lowVal , hpChoiceArray \\\n",
    "                            = xmlReadHyperOpt(xmlDict, valueType ,lowVal, hpChoiceArray, child2.tag, child3.tag, c3Text)\n",
    "                    ############################################################################\n",
    "                    # その他のタグ\n",
    "                    ############################################################################\n",
    "                    else:\n",
    "                        raise ValueError (\"【ERROR】タグが不正です：\" + child3.tag)\n",
    "\n",
    "    return algoStr , xmlDict , hyperOptFlag\n",
    "\n",
    "####################################\n",
    "##### ★バグがあるので未使用★\n",
    "##### ↓XML読み込みサブロジック(HyperOpt)↓\n",
    "####################################\n",
    "def xmlReadHyperOpt(inXmlDict, inValueType ,inLowVal, inChoiceArray, inC2tag, inC3tag, inC3Text):\n",
    "    xmlDict = inXmlDict\n",
    "    lowVal = inLowVal\n",
    "    choiceArray = inChoiceArray\n",
    "    \n",
    "    if(re.match('hp.uniformLow', inC3tag) or re.match('hp.loguniformLow', inC3tag)):\n",
    "        if(inValueType == TYPE_INT):\n",
    "            # int型の場合\n",
    "            lowVal = int(inC3Text)\n",
    "        elif(inValueType == TYPE_FLOAT):\n",
    "            # float型の場合\n",
    "            lowVal = float(inC3Text)\n",
    "        elif(inValueType == TYPE_DOUBLE):\n",
    "            # double型の場合\n",
    "            lowVal = double(inC3Text)\n",
    "    elif(re.match('hp.uniformHigh', inC3tag)):\n",
    "        if(inValueType == TYPE_INT):\n",
    "            # int型の場合\n",
    "            # xmlDictに設定\n",
    "            xmlDict[inC2tag] = hp.uniform(inC2tag, int(inLowVal), int(inC3Text))\n",
    "        elif(inValueType == TYPE_FLOAT):\n",
    "            # float型の場合\n",
    "            # xmlDictに設定\n",
    "            xmlDict[inC2tag] = hp.uniform(inC2tag, float(inLowVal), float(inC3Text))\n",
    "        elif(inValueType == TYPE_DOUBLE):\n",
    "            # double型の場合\n",
    "            # xmlDictに設定\n",
    "            xmlDict[inC2tag] = hp.uniform(inC2tag, double(inLowVal), double(inC3Text))\n",
    "    elif(re.match('hp.loguniformHigh', inC3tag)):\n",
    "        if(inValueType == TYPE_INT):\n",
    "            # int型の場合\n",
    "            xmlDict[inC2tag] = hp.loguniform(inC2tag, np.log(int(inLowVal)), np.log(int(inC3Text)))\n",
    "        elif(inValueType == TYPE_FLOAT):\n",
    "            # float型の場合\n",
    "            xmlDict[inC2tag] = hp.loguniform(inC2tag, np.log(float(inLowVal)), np.log(float(inC3Text)))\n",
    "        elif(inValueType == TYPE_DOUBLE):\n",
    "            # double型の場合\n",
    "            xmlDict[inC2tag] = hp.loguniform(inC2tag, np.log(double(inLowVal)), np.log(double(inC3Text)))\n",
    "    elif(re.match('hp.randint', inC3tag)):\n",
    "        # xmlDictに設定\n",
    "        xmlDict[inC2tag] = hp.randint(inC2tag, int(inC3Text))\n",
    "    elif(re.match('hp.choice*', inC3tag)):\n",
    "        if(inValueType == TYPE_INT):\n",
    "            # int型の場合\n",
    "            paramVal = int(inC3Text)\n",
    "        elif(inValueType == TYPE_FLOAT):\n",
    "            # float型の場合\n",
    "            paramVal = float(inC3Text)\n",
    "        elif(inValueType == TYPE_DOUBLE):\n",
    "            # double型の場合\n",
    "            paramVal = double(inC3Text)\n",
    "        elif(inValueType == TYPE_BOOL):\n",
    "            # bool型の場合\n",
    "            # まずは文字列で読みだしてからboolに変換する。\n",
    "            paramVal = str(inC3Text)\n",
    "            if(paramVal == \"True\"):\n",
    "                paramVal = True\n",
    "            elif(paramVal == \"False\"):\n",
    "                paramVal = False\n",
    "            else:\n",
    "                raise ValueError (\"【ERROR】値が不正です：\" + inC3tag)\n",
    "        else:\n",
    "            # string型の場合\n",
    "            paramVal = str(inC3Text)\n",
    "        choiceArray.append(paramVal)\n",
    "        # ラストのタグの場合にxmlDictに設定\n",
    "        if(inC3tag == \"hp.choiceLast\"):\n",
    "            xmlDict[inC2tag] = hp.choice(inC2tag, choiceArray)\n",
    "        \n",
    "    return xmlDict, lowVal, choiceArray\n",
    "\n",
    "####################################\n",
    "##### ↓ランクDFからディクショナリを生成する関数\n",
    "####################################\n",
    "def createDictFromRankDf(scoringName, nameArray, createDict, pipelines):\n",
    "    # 順位でソートしたアルゴリズムの配列\n",
    "    for elem in nameArray:\n",
    "        # パイプラインのループ\n",
    "        for pipe_name, pipeline in pipelines.items():\n",
    "            # 該当のアルゴリズムを検索\n",
    "            if(elem == pipe_name):\n",
    "                createDict[scoringName + MODEL_SPRIT + pipe_name] = pipeline\n",
    "                break\n",
    "\n",
    "        \n",
    "    return createDict\n",
    "\n",
    "# ####################################\n",
    "# ##### 【未使用】\n",
    "# ##### ↓不均衡調整を行う関数\n",
    "# ####################################\n",
    "# def unbalanceAdj(adjMode, X_inputDf, y_inputDf):\n",
    "#     print(\"=============================== unbalanceAdj START ===============================\")\n",
    "#     print(\"【第1引数】adjMode：\" ,adjMode)\n",
    "\n",
    "\n",
    "#     print(\"不均衡調整実施前：\",Counter(y_inputDf.as_matrix().ravel()))\n",
    "    \n",
    "#     if(adjMode == UNB_ADJ_NASI):\n",
    "#         # 不均衡調整モード＝なし の場合\n",
    "#         ret_XDf = X_inputDf\n",
    "#         ret_yDf = y_inputDf\n",
    "#         print(\"不均衡調整実施なし\")\n",
    "#     elif(adjMode == UNB_ADJ_UNDER):\n",
    "#         # 不均衡調整モード＝アンダーサンプリング の場合\n",
    "#         rus = RandomUnderSampler(random_state=RANDOM_STATE_VAL)\n",
    "#         X_under, y_under = rus.fit_sample(X_inputDf, y_inputDf)\n",
    "#         print(\"不均衡調整実施後(アンダーサンプリング)：\",Counter(y_under))\n",
    "#         ret_XDf = pd.DataFrame(X_under, columns= X_inputDf.columns.values)\n",
    "#         ret_yDf = pd.DataFrame(y_under, columns= y_inputDf.columns.values)\n",
    "#     elif(adjMode == UNB_ADJ_OVER):\n",
    "#         # 不均衡調整モード＝オーバーサンプリング の場合\n",
    "#         ros = RandomOverSampler(random_state=RANDOM_STATE_VAL)\n",
    "#         X_over, y_over = ros.fit_sample(X_inputDf, y_inputDf)\n",
    "#         print(\"不均衡調整実施後(オーバーサンプリング)：\",Counter(y_over))\n",
    "#         ret_XDf = pd.DataFrame(X_over, columns= X_inputDf.columns.values)\n",
    "#         ret_yDf = pd.DataFrame(y_over, columns= y_inputDf.columns.values)\n",
    "#     elif(adjMode == UNB_ADJ_SMOTE):\n",
    "#         # 不均衡調整モード＝SMOTE の場合\n",
    "#         smt = SMOTE(random_state=RANDOM_STATE_VAL)\n",
    "#         X_smt, y_smt = smt.fit_sample(X_inputDf, y_inputDf)\n",
    "#         print(\"不均衡調整実施後(SMOTE)：\",Counter(y_smt))\n",
    "#         ret_XDf = pd.DataFrame(X_smt, columns= X_inputDf.columns.values)\n",
    "#         ret_yDf = pd.DataFrame(y_smt, columns= y_inputDf.columns.values)\n",
    "#     else:\n",
    "#         # 不均衡調整モード＝その他 の場合\n",
    "#         raise ValueError (\"【ERROR】不均衡調整モードの値が不正です。：\" + adjMode)\n",
    "\n",
    "#     ####\n",
    "#     displayPrintDf(ret_XDf,\\\n",
    "#         \"++++++ ↓%sの先頭%d行(不均衡調整後X)↓ ++++++\",FILE_TYPE_NAME_TRAIN,PRINT_DF_LINENUM)\n",
    "#     print(ret_XDf.dtypes)\n",
    "\n",
    "#     displayPrintDf(ret_yDf,\\\n",
    "#         \"++++++ ↓%sの先頭%d行(不均衡調整後y)↓ ++++++\",FILE_TYPE_NAME_TRAIN,PRINT_DF_LINENUM)\n",
    "#     print(ret_yDf.dtypes)\n",
    "    \n",
    "#     return ret_XDf, ret_yDf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●関数の宣言(重いやつ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pink\n",
    "\n",
    "####################################\n",
    "##### ↓ディクショナリをキー入力させる関数\n",
    "####################################\n",
    "def getInputNumberKeyDict(inputDict, dispStr):\n",
    "    input_pipe_key_Dict=dict()\n",
    "    for index,key in enumerate(inputDict):\n",
    "        input_pipe_key_Dict[index+1] = key\n",
    "    # 末尾に中止を追加\n",
    "    input_pipe_key_Dict[STOP_NUMBER] = \"中止\"\n",
    "    \n",
    "    # アルゴリズム選択\n",
    "    algNumber = getInputNumberDic(dispStr, input_pipe_key_Dict)\n",
    "    if (algNumber == STOP_NUMBER):\n",
    "        raise RuntimeError (\"【ERROR】実行を中止しました。！\")\n",
    "        \n",
    "    retVal = inputDict.get(input_pipe_key_Dict[algNumber])\n",
    "    keyStr = input_pipe_key_Dict[algNumber]\n",
    "\n",
    "    return keyStr, retVal\n",
    "        \n",
    "####################################\n",
    "##### ↓ハイパーパラメータをXMLから取得する関数\n",
    "####################################\n",
    "def getHyperPara(algorithm, inputFileName):\n",
    "    readAlgo , readParaDict, hyperOptFlag = xmlRead(inputFileName)\n",
    "    \n",
    "    if(algorithm != readAlgo):\n",
    "        raise ValueError (\"【ERROR】XMLのアルゴリズムが不正です。\" + readAlgo)\n",
    "    \n",
    "    print(\"++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"XMLのアルゴリズム = \",readAlgo)\n",
    "    print(\"XMLのハイパーパラメータ = \",readParaDict)\n",
    "#     print(\"HyperOptFlag = \",hyperOptFlag)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++\")\n",
    "    return readParaDict, hyperOptFlag\n",
    "\n",
    "####################################\n",
    "##### ↓【最重要】ベストなモデルを選択して返す関数\n",
    "# 引数 訓練用の特徴量,訓練用の正解\n",
    "####################################\n",
    "def getBestModelClf(X_input, y_input, i_feature_num, i_feature_dir, i_cv_num):\n",
    "    \n",
    "    # デフォルトのモデル\n",
    "    prePredict, scoring , algorithm , defaultClfPipe, defaultRankDf, feature_num = \\\n",
    "        getBestModelDefaultClfLogic(X_input, y_input, i_feature_num, i_feature_dir)\n",
    "    \n",
    "    ## モデル最終決定用のディクショナリ\n",
    "    model_Dict=dict()\n",
    "\n",
    "    ############# ↓交差検証の場合↓ #############\n",
    "    if(conf_eval_method == K_FOLD_NUMBER):\n",
    "        # 交差検証の場合にのみパラメータチューニングを行う。\n",
    "        model_Dict[scoring + MODEL_SPRIT + algorithm] = defaultClfPipe\n",
    "        inputRankDf = defaultRankDf\n",
    "\n",
    "        ###### ↓wheileループ(パラメータチューニング)↓ ######\n",
    "        tuneNo = 0\n",
    "        while True:\n",
    "            inputFileName = getInputFileName(FILE_TYPE_HYPER_PARA_NAME,\n",
    "                                             \"\\r\\nこれ以上行わない場合は「c」を入力してください。\")\n",
    "\n",
    "            if(inputFileName == 'c'):\n",
    "                # ループを抜ける。\n",
    "                break\n",
    "            else:\n",
    "                print(\"=== パラメータチューニングを実行します。===\")\n",
    "                tuneNo = tuneNo + 1\n",
    "\n",
    "                try:\n",
    "                    # ハイパーパラメータチューニングを実施\n",
    "                    model_Dict, inputRankDf = getBestModelTuneClfLogic(prePredict, X_input,\n",
    "                                y_input, scoring , algorithm, defaultClfPipe, \n",
    "                                inputRankDf, tuneNo, inputFileName, model_Dict, feature_num, i_feature_dir,\n",
    "                                i_cv_num)\n",
    "                except ValueError as e:\n",
    "                    print(\"例外発生！：\\r\\n\" , e)\n",
    "                    sleep(1)\n",
    "        ###### ↑wheileループ(パラメータチューニング)↑ ######\n",
    "    ############# ↑交差検証の場合↑ #############\n",
    "    \n",
    "    ## モデル最終決定\n",
    "    if(len(model_Dict) > 1):\n",
    "        # パラメータチューニングした場合\n",
    "        print(\"パラメータチューニングあり\")\n",
    "        key, retPipe = getInputNumberKeyDict(model_Dict, \"モデル\")\n",
    "    else:\n",
    "        # パラメータチューニングしていない場合\n",
    "        print(\"パラメータチューニングなし\")\n",
    "        retPipe = defaultClfPipe\n",
    "\n",
    "    return retPipe\n",
    "\n",
    "\n",
    "# ####################################\n",
    "# ##### ★未使用★\n",
    "# ##### ↓HyperOptCVのサブロジック\n",
    "# ####################################\n",
    "# def hyperOptLogic(pipeline, paramDic, X_train, y_train, X_test, y_test, scoringName):\n",
    "#     pipeline.fit(X_train,\n",
    "#                 y_train)\n",
    "#     return -f1_score(y_test, pipeline.predict(X_test), pos_label=TARGET_PROB_CLASS_VALUE)\n",
    "\n",
    "# ####################################\n",
    "# ##### ★未使用★\n",
    "# ##### ↓HyperOptCVExeを実行して\n",
    "# #####   後のGridSearchCVに渡すパラメータを生成する。\n",
    "# ####################################\n",
    "# def hyperOptCVExe(pipeline, paramDic, X_input, y_input, scoringName):\n",
    "    \n",
    "#     # 元の訓練データを訓練用と検証用に分ける\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X_input,     # 特徴量\n",
    "#                                              y_input,                # 正解データ\n",
    "#                                              test_size=TEST_SIZE_VAL ,\n",
    "#                                              random_state=RANDOM_STATE_VAL)\n",
    "#     y_train_ravel = y_train.values.ravel()\n",
    "#     y_test_ravel = y_test.values.ravel()\n",
    "\n",
    "#     convParamDic = fmin(hyperOptLogic(pipeline, paramDic, X_train, y_train_ravel, X_test, y_test_ravel, scoringName), \\\n",
    "#                         paramDic, algo=tpe.suggest, max_evals=100)\n",
    "#     print(\"best estimate parameters = \", convParamDic)\n",
    "\n",
    "#     # GridSearchCVに渡すパラメータディクショナリを返す\n",
    "#     return convParamDic\n",
    "\n",
    "####################################\n",
    "##### ↓【最重要】ベストなモデルを選択して返す関数(Tune)\n",
    "# 引数 訓練用の特徴量,訓練用の正解\n",
    "####################################\n",
    "def getBestModelTuneClfLogic(prePredict, X_input, y_input, scoringName , algorithm, clfPipe, inputRankDf, \n",
    "    tuneNo, inputFileName, inputModelDct, i_feature_num, i_feature_dir, i_cv_num):\n",
    "    print(\"=============================== getBestModelTuneClfLogic START ===============================\")\n",
    "    print(\"【第1引数】prePredict：\" ,prePredict)\n",
    "    displayPrintDf(X_input,\"【第2引数】X_inputの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    displayPrintDf(y_input,\"【第3引数】y_inputの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    print(\"【第4引数】scoringName：\" ,scoringName)\n",
    "    print(\"【第5引数】algorithm：\" ,algorithm)\n",
    "    print(\"【第6引数】clfPipe：\" ,clfPipe)\n",
    "    displayPrintDf(inputRankDf,\"【第7引数】inputRankDfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    print(\"【第8引数】tuneNo：\" ,str(tuneNo))\n",
    "    print(\"【第9引数】inputFileName：\" ,inputFileName)\n",
    "    print(\"【第10引数】inputModelDct：\" ,inputModelDct)\n",
    "    print(\"【第11引数】i_feature_num：\" ,i_feature_num)\n",
    "    print(\"【第12引数】i_feature_dir：\" ,i_feature_dir)\n",
    "    print(\"【第13引数】i_cv_num：\" ,i_cv_num)\n",
    "\n",
    "    \n",
    "    # warning消し\n",
    "    #.as_matrix will be removed in a future version. Use .values instead.\n",
    "    #y_input_ravel = y_input.as_matrix().ravel()\n",
    "    y_input_ravel = y_input.values.ravel()\n",
    "    \n",
    "    g_param1 , hyperOptFlag = getHyperPara(algorithm, inputFileName)\n",
    "    \n",
    "#     if(hyperOptFlag):\n",
    "#         # TODO TypeError: 'numpy.float64' object is not callable が解決するまで封印\n",
    "#         # HyperOptCVを実行\n",
    "#         # GridSearchCVにつなげるためg_param1のキーを変換する。\n",
    "#         g_param1 = hyperOptCVExe(clfPipe,\n",
    "#             g_param1,\n",
    "#             X_input, y_input, scoringName)\n",
    "#         #####################################\n",
    "    \n",
    "    # n_jobs=-1とすると、CPUコア数の数だけ並列化される\n",
    "    estVal = GridSearchCV(estimator = clfPipe,\n",
    "                            param_grid = g_param1,\n",
    "                            n_jobs=-1,\n",
    "                            scoring=scoringName,\n",
    "                            cv=conf_grid_cv_num,\n",
    "                            return_train_score=False)\n",
    "    \n",
    "    # パイプラインディクショナリに当チューニングを追加\n",
    "    pipelineDct = {\n",
    "        algorithm + \"_tune\" + str(tuneNo) : estVal\n",
    "    }\n",
    "\n",
    "    print(\"############################################# k-fold #############################################\")\n",
    "    ### k-foldの結果を取得\n",
    "    k_fold_Df = \\\n",
    "        getKFoldResult(scoringName, pipelineDct, X_input, y_input_ravel, i_cv_num, True)\n",
    "    \n",
    "    \n",
    "    # 精度の大きい順に順位付けしてソートし直す。\n",
    "    k_fold_Df = getRankDf(k_fold_Df, '精度', 'algorithm', True)\n",
    "\n",
    "    #    0:algorithm  1:精度       2:順位\n",
    "    # → 0:順位       1:algorithm  2:精度\n",
    "    k_fold_Df = (k_fold_Df.iloc[:,[2]]).join(k_fold_Df.iloc[:,[0,1]])\n",
    "\n",
    "    # 【要改善】デフォルトDfをくっつけて\n",
    "    # 精度の大きい順に順位付けしてソートし直す。\n",
    "    k_fold_Df = pd.concat( [inputRankDf, k_fold_Df] )\n",
    "    k_fold_Df = getRankDf(k_fold_Df, '精度', 'algorithm', False)\n",
    "\n",
    "    # ファイル出力(フォルダ名には拡張子の「.xml」は含まない)\n",
    "    outDir = \"tune\" + str(tuneNo) + \"_\" + inputFileName[:-4]\n",
    "    fileOutput(k_fold_Df, outDir ,\n",
    "               str(i_feature_num) + \"_\" + PRE_PREDICT_K_FOLD + \n",
    "               \"_perf_\" + scoringName + \"_tune\" + str(tuneNo) + \".csv\",\n",
    "               len(k_fold_Df) , False, i_feature_dir)\n",
    "    # XMLファイルをコピー\n",
    "    shutil.copyfile(conf_xml_folder_path + inputFileName,\n",
    "                    conf_output_folder_path + i_feature_dir  + outDir + \"/\" + inputFileName)\n",
    "\n",
    "    # パイプラインディクショナリに載せ替え\n",
    "    for pipe_name, pipeline in inputModelDct.items():\n",
    "        # pipe_nameをスペースの前後で分解\n",
    "        scoring , modelName = pipe_name.split()\n",
    "        pipelineDct[modelName] = pipeline\n",
    "    \n",
    "    # アルゴリズムの順位ごとにパイプラインをディクショナリで保持\n",
    "    # warning消し\n",
    "    #.as_matrix will be removed in a future version. Use .values instead.\n",
    "#     modelDict = createDictFromRankDf(scoringName, k_fold_Df['algorithm'].as_matrix().ravel(),\n",
    "#         dict(), pipelineDct)\n",
    "    modelDict = createDictFromRankDf(scoringName, k_fold_Df['algorithm'].values.ravel(),\n",
    "        dict(), pipelineDct)\n",
    "\n",
    "    ## k-fold 描画\n",
    "    rankDfKfoldDict = dict()\n",
    "    rankDfKfoldDict[scoringName]=k_fold_Df\n",
    "    plotPerf(rankDfKfoldDict, K_FOLD_NUMBER)\n",
    "\n",
    "\n",
    "    return modelDict , k_fold_Df\n",
    "\n",
    "####################################\n",
    "##### ↓【最重要】ベストなモデルを選択して返す関数(デフォルト)\n",
    "# 引数 訓練用の特徴量,訓練用の正解\n",
    "####################################\n",
    "def getBestModelDefaultClfLogic(X_input, y_input, i_feature_num, i_feature_dir):\n",
    "    print(\"=============================== getBestModelClf START ===============================\")\n",
    "    displayPrintDf(X_input,\"【第1引数】X_inputの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    displayPrintDf(y_input,\"【第2引数】y_inputの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    print(\"【第3引数】i_feature_num：\" ,i_feature_num)\n",
    "    print(\"【第4引数】i_feature_dir：\" ,i_feature_dir)\n",
    "    #######################\n",
    "\n",
    "    # 元の訓練データを訓練用と検証用に分ける(HoldOut用)\n",
    "    inputTrainFeatureDf, inputTestFeatureDf, y_train, y_test = train_test_split(X_input,     # 特徴量\n",
    "                                                 y_input,    # 正解データ\n",
    "                                                 test_size=TEST_SIZE_VAL ,\n",
    "                                                 random_state=RANDOM_STATE_VAL)\n",
    "\n",
    "    print(\"===== スプリット後 =====\")\n",
    "    displayPrintDf(inputTrainFeatureDf,\"inputTrainFeatureDfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    displayPrintDf(y_train,\"y_trainの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    displayPrintDf(inputTestFeatureDf,\"inputTestFeatureDfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    displayPrintDf(y_test,\"y_testの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "\n",
    "    \n",
    "    \n",
    "    # 特徴量数の調整\n",
    "    X_clomns_num = len(X_input.columns)\n",
    "    if(i_feature_num <= X_clomns_num):\n",
    "        # DataFrameの特徴量の数がコンフィグ/コマンドの設定値以上の場合\n",
    "        # コンフィグ/コマンドの設定値を採用する\n",
    "        feature_num_val_adj = i_feature_num\n",
    "    else:\n",
    "        # DataFrameの特徴量の数がコンフィグ/コマンドの設定値よりも少ない場合\n",
    "        # DataFrameの特徴量の数を採用する\n",
    "        feature_num_val_adj = X_clomns_num\n",
    "        \n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "    print(\"調整後の特徴量の上限数 = \", feature_num_val_adj)\n",
    "    print(\"++++++++++++++++++++++++++++++++++++++++++++++++\")\n",
    "\n",
    "    # warning消し\n",
    "    #.as_matrix will be removed in a future version. Use .values instead.\n",
    "    #y_train_ravel = y_train.as_matrix().ravel()\n",
    "    #y_test_ravel = y_test.as_matrix().ravel()\n",
    "    #y_input_ravel = y_input.as_matrix().ravel()\n",
    "    y_train_ravel = y_train.values.ravel()\n",
    "    y_test_ravel = y_test.values.ravel()\n",
    "    y_input_ravel = y_input.values.ravel()\n",
    "    \n",
    "    \n",
    "\n",
    "    clf_pipe_Hout_Dict=dict()\n",
    "    clf_pipe_Kfold_Dict=dict()\n",
    "    rankDfKfoldDict = dict()\n",
    "    rankDfHoutDict = dict()\n",
    "\n",
    "    #####################################################\n",
    "    ################## ↓whileループ↓ ##################\n",
    "    #####################################################\n",
    "    selPerfFlag = False\n",
    "    while True:\n",
    "        perf_number = getInputNumberDic(\"性能評価指標\",perfDict)\n",
    "\n",
    "        # 'アルゴリズム選択へ進む'の場合\n",
    "        if perf_number == GO_NEXT:\n",
    "            if selPerfFlag == True:\n",
    "                # 性能評価指標が選択済みの場合\n",
    "                print(\"性能評価指標選択を終了します。\")\n",
    "                break;\n",
    "            else:\n",
    "                # 性能評価指標が未選択の場合\n",
    "                print(\"【ERROR】性能評価指標が未選択です。\")\n",
    "                continue\n",
    "        elif (perf_number == STOP_NUMBER):\n",
    "            raise RuntimeError (\"【ERROR】実行を中止しました。！\")\n",
    "\n",
    "        # 選択済みフラグON\n",
    "        selPerfFlag = True\n",
    "\n",
    "        # 性能評価ラベルの取得\n",
    "        perf_label = perfLabelDict.get(perf_number)\n",
    "        \n",
    "        # アルゴリズム取得\n",
    "        pipelines = getPipelines(feature_num_val_adj)\n",
    "\n",
    "        ############# ↓交差検証の場合↓ #############\n",
    "        if(conf_eval_method == K_FOLD_NUMBER):\n",
    "            print(\"############################################# k-fold #############################################\")\n",
    "            ### k-foldの結果を取得(この時点でのcvは少ない数値で行う。)\n",
    "            k_fold_Df = \\\n",
    "                getKFoldResult(perf_label, pipelines, X_input, y_input_ravel, CV_NUM_DEFAULT, False)\n",
    "\n",
    "            # 精度の大きい順に順位付けしてソートし直す。\n",
    "            k_fold_Df = getRankDf(k_fold_Df, '精度', 'algorithm', True)\n",
    "\n",
    "            #    0:algorithm  1:精度       2:順位\n",
    "            # → 0:順位       1:algorithm  2:精度\n",
    "            k_fold_Df = (k_fold_Df.iloc[:,[2]]).join(k_fold_Df.iloc[:,[0,1]])\n",
    "            fileOutput(k_fold_Df, \"default\" ,\n",
    "                       str(i_feature_num) + \"_\" + PRE_PREDICT_K_FOLD + \"_perf_\"+ perf_label \n",
    "                        +\".csv\", len(pipelines), False, i_feature_dir)\n",
    "\n",
    "            # アルゴリズムの順位ごとにパイプラインをディクショナリで保持\n",
    "    # warning消し\n",
    "    #.as_matrix will be removed in a future version. Use .values instead.\n",
    "#             clf_pipe_Kfold_Dict = createDictFromRankDf(perf_label, k_fold_Df['algorithm'].as_matrix().ravel(),\n",
    "#                 clf_pipe_Kfold_Dict, pipelines)\n",
    "            clf_pipe_Kfold_Dict = createDictFromRankDf(perf_label, k_fold_Df['algorithm'].values.ravel(),\n",
    "                clf_pipe_Kfold_Dict, pipelines)\n",
    "\n",
    "            ## k-fold 描画\n",
    "            rankDfKfoldDict[perf_label]=k_fold_Df\n",
    "            plotPerf(rankDfKfoldDict, K_FOLD_NUMBER)\n",
    "            \n",
    "        ############# ↓Hold Outの場合↓ #############\n",
    "        else:\n",
    "\n",
    "            print(\"############################################# HoldOut #############################################\")\n",
    "            ### HoldOutの結果を取得\n",
    "            rankDf = \\\n",
    "                getHoldOutResult(perf_number, perf_label, pipelines, \\\n",
    "                    inputTrainFeatureDf, y_train_ravel, inputTestFeatureDf, y_test_ravel)\n",
    "        \n",
    "            # testの値が大きい順に順位付けしてソートし直す。\n",
    "            rankDf = getRankDf(rankDf, rankDf.columns.values[1], 'algorithm', True)\n",
    "\n",
    "\n",
    "            #    0:algorithm  1:train-評価  2:test-評価  3:順位\n",
    "            # → 0:順位       1:algorithm   2:test-評価  3:train-評価    \n",
    "            rankDf = (rankDf.iloc[:,[3]]).join((rankDf.iloc[:,[0]])).join((rankDf.iloc[:,[2]])) \\\n",
    "                .join(rankDf.iloc[:,[1]])\n",
    "\n",
    "            fileOutput(rankDf, \"default\", \n",
    "                       str(i_feature_num) + \"_\" + PRE_PREDICT_HOLDOUT+ \"_perf_\"+ perf_label +\n",
    "                       \".csv\", len(pipelines), False, i_feature_dir)\n",
    "        \n",
    "            # アルゴリズムの順位ごとにパイプラインをディクショナリで保持\n",
    "            # warning消し\n",
    "            #.as_matrix will be removed in a future version. Use .values instead.\n",
    "#             clf_pipe_Hout_Dict = createDictFromRankDf(perf_label, rankDf['algorithm'].as_matrix().ravel(),\n",
    "#                 clf_pipe_Hout_Dict, pipelines)\n",
    "            clf_pipe_Hout_Dict = createDictFromRankDf(perf_label, rankDf['algorithm'].values.ravel(),\n",
    "                clf_pipe_Hout_Dict, pipelines)\n",
    "\n",
    "    \n",
    "            ## HoldOut 描画\n",
    "            rankDfHoutDict[perf_label]=rankDf\n",
    "            plotPerf(rankDfHoutDict, H_OUT_NUMBER)\n",
    "    #####################################################\n",
    "    ################## ↑whileループ↑ ##################\n",
    "    #####################################################\n",
    "\n",
    "    if(conf_eval_method == K_FOLD_NUMBER):\n",
    "        # k-foldの場合\n",
    "        modelDict = clf_pipe_Kfold_Dict\n",
    "        prePredict = PRE_PREDICT_K_FOLD\n",
    "        selRankDict = rankDfKfoldDict\n",
    "    else:\n",
    "        # HoldOutの場合\n",
    "        modelDict = clf_pipe_Hout_Dict\n",
    "        prePredict = PRE_PREDICT_HOLDOUT\n",
    "        selRankDict = rankDfHoutDict\n",
    "\n",
    "    ##################\n",
    "    ### アルゴリズムをコンソールから選択して学習モデル返却\n",
    "    # パイプラインのキーをディクショナリで番号で管理\n",
    "    keyStr, selClfPipe = getInputNumberKeyDict(modelDict, \"モデル\")\n",
    "    scoring , algorithmVal = keyStr.split()\n",
    "    \n",
    "    selRankDf = selRankDict[scoring]\n",
    "    # 指定したアルゴリズムの行だけ抜き出す(where句でhitしなかった行はnaなのでdropする)\n",
    "    selRankDf = selRankDf.where(selRankDf['algorithm'] == algorithmVal).dropna()\n",
    "    \n",
    "    return prePredict, scoring , algorithmVal , selClfPipe , selRankDf, feature_num_val_adj\n",
    "\n",
    "\n",
    "####################################\n",
    "##### ↓パイプライン取得\n",
    "####################################\n",
    "def getPipelines(i_feature_num_val_adj):\n",
    "    ### アルゴリズムのパイプライン ###\n",
    "    # set pipelines for different algorithms\n",
    "    pipelines = {\n",
    "# 欠番\n",
    "#             # :K近傍法\n",
    "#             'knn(K近傍法)':\n",
    "#                 Pipeline([('scl',StandardScaler()),\n",
    "#                           ('reduct', PCA(n_components=i_feature_num_val_adj, random_state=RANDOM_STATE_VAL)),\n",
    "#                           ('est',KNeighborsClassifier())]),\n",
    "\n",
    "        # 1:ロジスティック回帰\n",
    "        ALG_NAME_LOGISTIC:\n",
    "            Pipeline([('scl',StandardScaler()),\n",
    "                      # TODO 必要に応じて次元圧縮復活\n",
    "                      #('reduct', PCA(n_components=i_feature_num_val_adj,random_state=RANDOM_STATE_VAL)),\n",
    "                      # 「log_tune_TPOT.xml」 に準拠\n",
    "                      ('est',LogisticRegression(C=15.0, random_state=RANDOM_STATE_VAL))]),\n",
    "                      # デフォルト\n",
    "                      #('est',LogisticRegression(random_state=RANDOM_STATE_VAL))]),\n",
    "\n",
    "    # ×欠番 (大量データだとやたら計算が遅い)\n",
    "    #     # :SVC(kernel='rbf' RBFカーネルのパラメータ: γは, 以下のRBFカーネルの式の中で現れます)\n",
    "    #     'rsvc(SVC)':\n",
    "    #         Pipeline([('scl',StandardScaler()),\n",
    "    #                   ('reduct', PCA(n_components=i_feature_num_val_adj, random_state=RANDOM_STATE_VAL)),\n",
    "    #                   ('est',SVC(C=1.0,kernel='rbf',class_weight='balanced',random_state=RANDOM_STATE_VAL, probability =True))]),\n",
    "\n",
    "    # ×欠番\n",
    "    # AttributeError: 'LinearSVC' object has no attribute 'predict_proba'\n",
    "    #\n",
    "    #     # LinearSVC:inearSVCはカーネルが線形カーネルの場合に特化したSVMであり, \n",
    "    #     # 計算が高速だったり, 他のSVMにはないオプションが指定できたりする.\n",
    "    #     'lsvc(LinearSVC)':\n",
    "    #         Pipeline([('scl',StandardScaler()),\n",
    "    #                   #('reduct', PCA(n_components=i_feature_num_val_adj, random_state=RANDOM_STATE_VAL)),\n",
    "    #                   ('est',LinearSVC(C=1.0,class_weight='balanced',random_state=RANDOM_STATE_VAL))]),\n",
    "\n",
    "#         # :ランダムフォレスト(精度が低いので封印)\n",
    "#         ALG_NAME_RANDOMFOREST:\n",
    "#             Pipeline([('scl',StandardScaler()),\n",
    "#                       ('reduct', PCA(n_components=i_feature_num_val_adj, random_state=RANDOM_STATE_VAL)),\n",
    "#                       ('est',RandomForestClassifier(random_state=RANDOM_STATE_VAL))]),\n",
    "# 欠番\n",
    "#             # :勾配ブースティング\n",
    "#             'gb(勾配ブースティング)':\n",
    "#                 Pipeline([('scl',StandardScaler()),\n",
    "#                           ('reduct', PCA(n_components=i_feature_num_val_adj, random_state=RANDOM_STATE_VAL)),\n",
    "#                           ('est',GradientBoostingClassifier(max_depth=10,random_state=RANDOM_STATE_VAL))]),\n",
    "\n",
    "        # 2:多層パーセプトロン（MLP）\n",
    "        ALG_NAME_MLP:\n",
    "            Pipeline([('scl',StandardScaler()),\n",
    "                      # TODO 必要に応じて次元圧縮復活\n",
    "                      #('reduct', PCA(n_components=i_feature_num_val_adj, random_state=RANDOM_STATE_VAL)),\n",
    "                      # デフォルト\n",
    "                      # TODO warningが出たらmax_iterの値を増やすこと！！\n",
    "                      ('est',MLPClassifier(max_iter=600,\n",
    "                                           hidden_layer_sizes=(100,30,10),\n",
    "                                           random_state=RANDOM_STATE_VAL))]),\n",
    "        # 3:XGBoost\n",
    "        ALG_NAME_XGB:\n",
    "#            Pipeline([('scl',StandardScaler()),\n",
    "            Pipeline([\n",
    "                      # TODO 必要に応じて次元圧縮復活\n",
    "                      #('reduct', PCA(n_components=i_feature_num_val_adj, random_state=RANDOM_STATE_VAL)),\n",
    "                      # xg_tune_TPOT_mod.xml\n",
    "                      ('est',xgb.XGBClassifier(max_depth=5,random_state=RANDOM_STATE_VAL)\n",
    "                      # デフォルト\n",
    "                      #('est',xgb.XGBClassifier(max_depth=10,random_state=RANDOM_STATE_VAL)\n",
    "                     )])\n",
    "    #　ハイパーパラメータ調整\n",
    "    #                   ('est',GridSearchCV(estimator = xgb.XGBClassifier(\n",
    "    #                                             random_state=RANDOM_STATE_VAL),\n",
    "    #                         param_grid = xg_param1,n_jobs=-1,scoring=perf_label,cv=cv_num)  #cvのデフォルトは3\n",
    "    #                   )])\n",
    "    # 【重要】 デフォルトのパラメータ\n",
    "    #                   ('est',xgb.XGBClassifier(max_depth=10,\n",
    "    #                                            random_state=RANDOM_STATE_VAL)\n",
    "    #                  )])\n",
    "\n",
    "    }\n",
    "    return pipelines\n",
    "    \n",
    "    \n",
    "####################################\n",
    "##### ↓精度の描画を行う関数\n",
    "####################################\n",
    "def plotPerf(rankDict, evalNumber):\n",
    "    if(evalNumber == K_FOLD_NUMBER):\n",
    "        # k-foldの場合\n",
    "        colorlist=['red']\n",
    "        titleHeader = \"k-fold :  \"\n",
    "    else:\n",
    "        # HoldOutの場合\n",
    "        colorlist=['red','blue']\n",
    "        titleHeader = \"HoldOut :  \"\n",
    "    \n",
    "    loopIndex = 0\n",
    "    for key,valueDf in rankDict.items():\n",
    "        # カラム「アルゴリズム」から右側だけ切り出す\n",
    "        pltDf = valueDf.iloc[:,1:]\n",
    "        \n",
    "        # ホールドアウトの場合\n",
    "        if(evalNumber == H_OUT_NUMBER):\n",
    "            # 列名変更\n",
    "            pltDf = pltDf.rename(columns={pltDf.columns.values[2]: 'train', pltDf.columns.values[1]: 'test'})\n",
    "\n",
    "        # アルゴリズムをindexに指定\n",
    "        pltDf = pltDf.set_index(pltDf.columns.values[0])\n",
    "        # 描画\n",
    "        pltDf.plot.bar(color=colorlist)\n",
    "        plt.title(titleHeader + key, fontsize=PLT_TITLE_FONT_SIZE)\n",
    "        plt.ylabel('精度', fontsize=PLT_LABEL_FONT_SIZE)\n",
    "        plt.xlabel('アルゴリズム', fontsize=PLT_LABEL_FONT_SIZE)\n",
    "        minVal = pltDf[pltDf.columns.values[0]].min()\n",
    "        plt.ylim([(minVal - 0.05) , 1.0])   # y軸の最小メモリは最小値よりも0.05低い値\n",
    "        plt.legend(bbox_to_anchor=(1.1, 1), loc='upper left')   #凡例を左下\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "####################################\n",
    "##### ↓k-foldの結果を返す関数\n",
    "####################################\n",
    "def getKFoldResult(perf_label, pipelines, X_input, y_input_ravel, i_cv_num, gridSearchFlag):\n",
    "    print(\"=============================== getKFoldResult START ===============================\")\n",
    "    ret_k_fold_Df = pd.DataFrame(None,\\\n",
    "                     columns=['精度'])\n",
    "    ##### ↓k-foldのforループ↓ #####\n",
    "    pipe_index = 0\n",
    "    for pipe_name, pipeline in pipelines.items():\n",
    "        pipe_index = pipe_index + 1\n",
    "        print(\"===\" + pipe_name + \"===\")\n",
    "        # 学習させる\n",
    "        pipeline.fit(X_input, y_input_ravel)\n",
    "# TODO cross_validateは、cross_val_scoreよりも便利らしいが、いったんここはcross_val_scoreで。\n",
    "        cv_results = cross_val_score(pipeline,\n",
    "                         X_input,\n",
    "                         y_input_ravel,\n",
    "                         cv=i_cv_num,\n",
    "                         scoring=perf_label)\n",
    "        ret_k_fold_Df.loc[pipe_name] = cv_results.mean()\n",
    "\n",
    "        # TODO warning消したい。\n",
    "        #C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.  % self.max_iter, ConvergenceWarning)\n",
    "        #C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.  if diff:\n",
    "\n",
    "####'fit_time',score_time','test_score', 'train_score'\n",
    "# TODO cross_validateは、cross_val_scoreよりも便利らしいが、いったんここはcross_val_scoreで。\n",
    "#         cv_results = cross_validate(pipeline,\n",
    "#                          X_input,\n",
    "#                          y_input_ravel,\n",
    "#                          cv=cv_num,\n",
    "#                          scoring=perf_label)\n",
    "#         ret_k_fold_Df.loc[pipe_name] = cv_results['test_score'].mean()\n",
    "#         print(\"平均学習時間:\", cv_results['fit_time'].mean())\n",
    "#         print(\"平均評価時間:\", cv_results['score_time'].mean())\n",
    "        \n",
    "        if(gridSearchFlag == True):\n",
    "            print(\"best_params：\",pipeline.best_params_)\n",
    "    ##### ↑k-foldのforループ↑ #####\n",
    "    \n",
    "    \n",
    "    return ret_k_fold_Df\n",
    "\n",
    "\n",
    "####################################\n",
    "##### ↓HoldOutの結果を返す関数\n",
    "####################################\n",
    "def getHoldOutResult(perf_number, perf_label, pipelines, X_train, y_train_ravel, X_test, y_test_ravel):\n",
    "    \n",
    "    # fit & evaluation\n",
    "    score_item_train = {}\n",
    "    score_item_test = {}\n",
    "\n",
    "    ##### ↓HoldOutのforループ↓ #####\n",
    "    pipe_index = 0\n",
    "    for pipe_name, pipeline in pipelines.items():\n",
    "        pipe_index = pipe_index + 1\n",
    "        print(\"===\" + pipe_name + \"===\")\n",
    "\n",
    "        # 学習させる\n",
    "        pipeline.fit(X_train, y_train_ravel)\n",
    "\n",
    "        # 【accuracy_score(正解率)】\n",
    "        if perf_number == ACCURACY_NUMBER:\n",
    "            score_item_train[(pipe_name,'train_' + perf_label)] = accuracy_score(y_train_ravel, \n",
    "                                                            pipeline.predict(X_train))\n",
    "            score_item_test[(pipe_name,'test_' + perf_label)] = accuracy_score(y_test_ravel, \n",
    "                                                            pipeline.predict(X_test))\n",
    "        # 【precision_score(適合率)】\n",
    "        elif perf_number == PRECISION_NUMBER:\n",
    "            score_item_train[(pipe_name,'train_' + perf_label)] = precision_score(y_train_ravel, \n",
    "                                                            pipeline.predict(X_train))\n",
    "            score_item_test[(pipe_name,'test_' + perf_label)] = precision_score(y_test_ravel, \n",
    "                                                            pipeline.predict(X_test))\n",
    "        # 【recall_score(再現率)】\n",
    "        elif perf_number == RECALL_NUMBER:\n",
    "            score_item_train[(pipe_name,'train_' + perf_label)] = recall_score(y_train_ravel, \n",
    "                                                            pipeline.predict(X_train))\n",
    "            score_item_test[(pipe_name,'test_' + perf_label)] = recall_score(y_test_ravel, \n",
    "                                                            pipeline.predict(X_test))\n",
    "        # 【f1_score(F値)】\n",
    "        elif perf_number == F1_NUMBER:\n",
    "            score_item_train[(pipe_name,'train_' + perf_label)] = f1_score(y_train_ravel, \n",
    "                                                            pipeline.predict(X_train))\n",
    "            #デフォルトでpos_labelは1\n",
    "            score_item_test[(pipe_name,'test_' + perf_label)] = f1_score(y_test_ravel, \n",
    "                                                            pipeline.predict(X_test))\n",
    "            #デフォルトでpos_labelは1\n",
    "        # 【AUC】\n",
    "        elif perf_number == AUC_NUMBER:\n",
    "            score_item_train[(pipe_name,'train_' + perf_label)] = roc_auc_score(y_train_ravel, \n",
    "                                                            pipeline.predict_proba(X_train)[:,TARGET_PROB_CLASS_VALUE])\n",
    "        # ↓こう書いても一緒\n",
    "#                                                            pipeline.predict_proba(X_train)[:,[TARGET_PROB_CLASS_VALUE]])\n",
    "            score_item_test[(pipe_name,'test_' + perf_label)] = roc_auc_score(y_test_ravel, \n",
    "                                                            pipeline.predict_proba(X_test)[:,TARGET_PROB_CLASS_VALUE])\n",
    "        # ↓こう書いても一緒\n",
    "#                                                             pipeline.predict_proba(X_test)[:,[TARGET_PROB_CLASS_VALUE]])\n",
    "        # あり得ないルート\n",
    "        else:\n",
    "            raise ValueError (\"【ERROR】性能評価指標の値が不正です。\")\n",
    "    ##### ↑HoldOutのforループ↑ #####\n",
    "\n",
    "    \n",
    "    ###### ランキング出力\n",
    "    ## unstack ： 行から列へのピボット \n",
    "    ##            インデックスが階層化されている場合にかぎり有効です。\n",
    "    ##          Series→DataFrameになる。\n",
    "    # TODO\n",
    "    # warning消したい(f1で発生する。AUCでは発生しない。)\n",
    "    #C:\\Users\\USER\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.  if diff:\n",
    "    retRankDf = (pd.Series(score_item_train).unstack()).join(pd.Series(score_item_test).unstack())\n",
    "\n",
    "    return retRankDf\n",
    "\n",
    "####################################\n",
    "##### ↓順位を付与してソートしたDFを返す関数\n",
    "####################################\n",
    "def getRankDf(inputDf, sortColumnName, oldIndexChangeName, firstTimeFlag):\n",
    "    retRankDf = inputDf\n",
    "    # 精度の大きい順に順位付けしてソートし直す。\n",
    "    retRankDf['順位'] = (inputDf.rank(method='min',ascending=False)[sortColumnName]).astype('int')\n",
    "    retRankDf = retRankDf.sort_values('順位', ascending=True)\n",
    "    \n",
    "    retRankDf = retRankDf.reset_index(drop=not(firstTimeFlag))\n",
    "    if(firstTimeFlag):\n",
    "        # 初回の場合\n",
    "        # 列名の旧indexをoldIndexChangeNameに変更\n",
    "        retRankDf = retRankDf.rename(columns={'index': oldIndexChangeName})\n",
    "        \n",
    "    return retRankDf\n",
    "        \n",
    "####################################\n",
    "##### ↓予測して結果を返す関数\n",
    "# 引数 学習済みモデル,検証用データの特徴量、予測種別、予測確率index\n",
    "####################################\n",
    "def getPredictResultDf(sumi_clf_pipeline, inputTestFeatureDf, predictType, probIndex):\n",
    "    print(\"=============================== getPredictResultDf START ===============================\")\n",
    "    print(\"【第1引数】sumi_clf_pipeline：\", sumi_clf_pipeline)\n",
    "    displayPrintDf(inputTestFeatureDf,\"【第2引数】inputTestFeatureDfの先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    print(\"【第3引数】predictType：\", predictType)\n",
    "    print(\"【第4引数】probIndex：\", probIndex)\n",
    "    \n",
    "    if(predictType == PREDICTTYPE_LABEL):\n",
    "        # クラスのラベル\n",
    "        # <class 'numpy.ndarray'>の1次元配列\n",
    "        retArray = sumi_clf_pipeline.predict(inputTestFeatureDf)\n",
    "        #colName = 'Class'\n",
    "        colName = 'Survived'\n",
    "    else:\n",
    "        # 予測確率\n",
    "        # <class 'numpy.ndarray'>の2次元配列\n",
    "        retArray = sumi_clf_pipeline.predict_proba(inputTestFeatureDf)[:,probIndex]\n",
    "        colName = conf_col_proba\n",
    "        \n",
    "    # DataFrameに変換\n",
    "    retDf = pd.DataFrame(data=retArray, columns=[colName])\n",
    "    displayPrintDf(retDf,\"予測結果の先頭%d行：\" ,None,PRINT_DF_LINENUM)\n",
    "    return retDf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "●メイン処理++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#gray\n",
    "def main():\n",
    "    # ●メイン処理-データ取得・加工処理 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "    ################################################################################\n",
    "    # 全般的にfor文はlambda式で書き直せるものはそうしたいところだが、\n",
    "    # 時間的にアレなので、余裕があればやる。\n",
    "    ################################################################################\n",
    "    global feature_num_lim_val\n",
    "\n",
    "\n",
    "    # cross validation(10固定にしようと思ったが、時間がかかりすぎる可能性があるのでやめた。)\n",
    "    ###cv_num =  int(getSettingValue(conf_reha_cv_num, \"10\", conf_exe_mode))\n",
    "    cv_num =  conf_cv_num\n",
    "\n",
    "    print(\"cv_num = %d\" % cv_num)\n",
    "\n",
    "    # pklファイル名リスト取得\n",
    "    pkl_file_list=os.listdir(conf_pkl_folder_path)\n",
    "    print(\"pkl_file_list = \", pkl_file_list)\n",
    "\n",
    "\n",
    "    # pkl自動削除モードONの場合\n",
    "    if(conf_pkl_del_mode == FLAG_ON):\n",
    "        print(\"pkl自動削除モード=ON\")\n",
    "        #pklファイルを削除する。\n",
    "        for pkl_file in pkl_file_list:\n",
    "            os.remove(conf_pkl_folder_path + pkl_file)\n",
    "        # 自動削除後に再度pklファイル名リスト取得\n",
    "        pkl_file_list=os.listdir(conf_pkl_folder_path)\n",
    "\n",
    "\n",
    "    if(len(pkl_file_list) > 0):\n",
    "        print(\"************************************* pklあり(2回目以降) *************************************\")\n",
    "        print(\"== pklファイルから読み込む ==\")\n",
    "        with open(conf_pkl_folder_path + 'trainFeatureDf.pkl', 'rb') as DFfile:\n",
    "            trainFeatureDf = pickle.load(DFfile)\n",
    "        with open(conf_pkl_folder_path + 'testFeatureDf.pkl', 'rb') as DFfile:\n",
    "            testFeatureDf = pickle.load(DFfile)\n",
    "        with open(conf_pkl_folder_path + 'y_train.pkl', 'rb') as DFfile:\n",
    "            y_train = pickle.load(DFfile)\n",
    "        with open(conf_pkl_folder_path + 'idDf.pkl', 'rb') as DFfile:\n",
    "            idDf = pickle.load(DFfile)\n",
    "    else:\n",
    "        print(\"************************************* pklなし(初回実行) *************************************\")\n",
    "\n",
    "        # 特殊カラム対象の決定 ########################################################\n",
    "        #### oneHotEncoderExe対象のカラム名を取得してohe_clm_listに保持する。\n",
    "        ohe_clm_list = getFileValueList(conf_one_hot_encode_column_sitei, conf_input_folder_path,\\\n",
    "            getSettingValue(conf_reha_one_hot_encode_column, \"OneHotEncoderColumns.dat\", conf_exe_mode))\n",
    "\n",
    "        #### 年月日時分秒対象のカラム名を取得してdatetime_clm_listに保持する。\n",
    "        datetime_clm_list = getFileValueList(conf_datetime_column_sitei, conf_input_folder_path,\\\n",
    "            getSettingValue(conf_reha_datetime_column, \"DateTime.dat\", conf_exe_mode))\n",
    "\n",
    "        # 訓練用データcsvファイルを読み込みんでDfを返す。\n",
    "        trainInputDf = getDfByReadCSV(FILE_TYPE_NAME_TRAIN, conf_exe_mode,\\\n",
    "            conf_reha_train_input_file , ohe_clm_list, datetime_clm_list, conf_datetime_column_sitei)\n",
    "\n",
    "        # 検証用データcsvファイルを読み込みんでDfを返す。\n",
    "        testInputDf = getDfByReadCSV(FILE_TYPE_NAME_TEST, conf_exe_mode,\\\n",
    "            conf_reha_test_input_file , ohe_clm_list, datetime_clm_list, conf_datetime_column_sitei)\n",
    "\n",
    "        print(\"# ======================================================================================================\")\n",
    "        print(\"# ======================================================================================================\")\n",
    "        print(\"# ======================================================================================================\")\n",
    "\n",
    "        ######### 訓練用データ  ########################################################\n",
    "        # 1列目(ID)と2列目(クラス変数)を削除することにより、第3カラム以降(特徴量)のみ残す\n",
    "        trainFeatureDf = trainInputDf.drop(columns=[trainInputDf.columns.values[0], trainInputDf.columns.values[CLASS_COL_INDEX]],axis=1)\n",
    "\n",
    "        # OneHotEncoder\n",
    "        trainFeatureDf = oneHotEncoderExe(FILE_TYPE_NAME_TRAIN, ohe_clm_list, trainFeatureDf, conf_one_hot_encode_column_sitei)\n",
    "\n",
    "        # 時刻を経過時間に変換\n",
    "        ##trainFeatureDf = getConvTimeDf(FILE_TYPE_NAME_TRAIN, trainFeatureDf)\n",
    "\n",
    "        # 欠損値を埋める(平均値で埋める)\n",
    "        displayPrintDf(trainFeatureDf, \"++++++ ↓%sの欠損埋め前先頭%d行↓ ++++++\",FILE_TYPE_NAME_TRAIN,PRINT_DF_LINENUM)\n",
    "        trainFeatureDf = trainFeatureDf.fillna(trainFeatureDf.mean())\n",
    "        displayPrintDf(trainFeatureDf, \"++++++ ↓%sの欠損埋め後先頭%d行↓ ++++++\",FILE_TYPE_NAME_TRAIN,PRINT_DF_LINENUM)\n",
    "\n",
    "\n",
    "        # # ガード処理。\n",
    "        # # 以下のエラーが出るときがある。\n",
    "        # # ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "        # # これを避けるためには、入力データからNaNや無限大の列を除去する必要がある。\n",
    "        # trainFeatureDf = trainFeatureDf.drop(trainFeatureDf.columns[np.isnan(trainFeatureDf).any()], axis=1)\n",
    "        # displayPrintDf(trainFeatureDf,\"ガード処理後の%sの先頭%d行：\" ,FILE_TYPE_NAME_TRAIN,PRINT_DF_LINENUM)\n",
    "\n",
    "        print(\"# ======================================================================================================\")\n",
    "        print(\"# ======================================================================================================\")\n",
    "        print(\"# ======================================================================================================\")\n",
    "        ######### 検証用データ  ########################################################\n",
    "\n",
    "        # 1列目(ID)と2列目(クラス変数)を削除することにより、第3カラム以降(特徴量)のみ残す\n",
    "        testFeatureDf = testInputDf.drop(columns=[testInputDf.columns.values[0], testInputDf.columns.values[CLASS_COL_INDEX]],axis=1)\n",
    "\n",
    "        # OneHotEncoder\n",
    "        testFeatureDf = oneHotEncoderExe(FILE_TYPE_NAME_TEST, ohe_clm_list, testFeatureDf, conf_one_hot_encode_column_sitei)\n",
    "\n",
    "        # 時刻を経過時間に変換\n",
    "        ##testFeatureDf = getConvTimeDf(FILE_TYPE_NAME_TEST, testFeatureDf)\n",
    "\n",
    "        # 欠損値を埋める(平均値で埋める)\n",
    "        displayPrintDf(testFeatureDf, \"++++++ ↓%sの欠損埋め前先頭%d行↓ ++++++\",FILE_TYPE_NAME_TEST,PRINT_DF_LINENUM)\n",
    "        testFeatureDf = testFeatureDf.fillna(testFeatureDf.mean())\n",
    "        displayPrintDf(testFeatureDf, \"++++++ ↓%sの欠損埋め後先頭%d行↓ ++++++\",FILE_TYPE_NAME_TEST,PRINT_DF_LINENUM)\n",
    "\n",
    "\n",
    "        # # ガード処理。\n",
    "        # # 標準化の際に以下のエラーが出るときがある。\n",
    "        # # ValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "        # # これを避けるためには、入力データからNaNや無限大の列を除去する必要がある。\n",
    "        # testFeatureDf = testFeatureDf.drop(testFeatureDf.columns[np.isnan(testFeatureDf).any()], axis=1)\n",
    "        # displayPrintDf(testFeatureDf,\"ガード処理後の%sの先頭%d行：\" ,FILE_TYPE_NAME_TEST,PRINT_DF_LINENUM)\n",
    "\n",
    "        ###############################################################\n",
    "        # 正解データ\n",
    "        y_train = trainInputDf.iloc[:,[CLASS_COL_INDEX]]\n",
    "\n",
    "        # 試験用データのIDのカラムを取り出してjoinに備える。\n",
    "        idDf = testInputDf.iloc[:,[0]]\n",
    "\n",
    "        # 不均衡調整(モデルに課題がなければやる必要はない)\n",
    "        # バグってる気がするので封印\n",
    "        # trainFeatureDf, y_train = unbalanceAdj(conf_unbalance_adj_mode, trainFeatureDf, y_train)\n",
    "\n",
    "        print(\"== pklファイルに残す ==\")\n",
    "        with open(conf_pkl_folder_path + 'trainFeatureDf.pkl', 'wb') as DFfile:\n",
    "            pickle.dump(trainFeatureDf, DFfile, protocol=2)\n",
    "        with open(conf_pkl_folder_path + 'testFeatureDf.pkl', 'wb') as DFfile:\n",
    "            pickle.dump(testFeatureDf, DFfile, protocol=2)\n",
    "        with open(conf_pkl_folder_path + 'y_train.pkl', 'wb') as DFfile:\n",
    "            pickle.dump(y_train, DFfile, protocol=2)\n",
    "        with open(conf_pkl_folder_path + 'idDf.pkl', 'wb') as DFfile:\n",
    "            pickle.dump(idDf, DFfile, protocol=2)\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    ############################# ↓メインループ↓ #############################\n",
    "    ############################################################################\n",
    "    continueFlag = True\n",
    "    feature_num_list = list()\n",
    "    while continueFlag:\n",
    "        print(\"*****************************************************************************************************************\")\n",
    "        print(\"feature_num_lim_val = %d : START\" % feature_num_lim_val)\n",
    "        print(\"*****************************************************************************************************************\")\n",
    "\n",
    "        #####################\n",
    "        # 選択後の訓練用データのpklファイルパス\n",
    "        train_s_pkl_file = conf_pkl_folder_path + 'trainFeatureDf_s_' + str(feature_num_lim_val) + '.pkl'\n",
    "\n",
    "        # 選択後の訓練用データのpklファイルの有無判定\n",
    "        if(os.path.isfile(train_s_pkl_file)):\n",
    "            # 選択後の訓練用データのpklあり\n",
    "            print(\"++ \" + train_s_pkl_file + \" あり ++\")\n",
    "\n",
    "            # 訓練用データの特徴量をロード\n",
    "            with open(train_s_pkl_file, 'rb') as DFfile:\n",
    "                trainFeatureDf_s = pickle.load(DFfile)\n",
    "        else:\n",
    "            # 選択後の訓練用データのpklなし\n",
    "            print(\"++ \" + train_s_pkl_file + \" なし ++\")\n",
    "\n",
    "            # 特徴量選択\n",
    "            trainFeatureDf_s = getSelectorFeature(FILE_TYPE_NAME_TRAIN, trainFeatureDf, \\\n",
    "                 y_train, feature_num_lim_val)\n",
    "\n",
    "            # 訓練用データの特徴量をダンプ(HyperOptExe用にダンプする)\n",
    "            with open(train_s_pkl_file, 'wb') as DFfile:\n",
    "                pickle.dump(trainFeatureDf_s, DFfile, protocol=2)\n",
    "\n",
    "        #####################\n",
    "        # 選択後の検証用データのpklファイルパス\n",
    "        test_s_pkl_file = conf_pkl_folder_path + 'testFeatureDf_s_' + str(feature_num_lim_val) + '.pkl'\n",
    "\n",
    "        # 選択後の検証用データのpklファイルの有無判定\n",
    "        if(os.path.isfile(test_s_pkl_file)):\n",
    "            # 選択後の検証用データのpklあり\n",
    "            print(\"++ \" + test_s_pkl_file + \" あり ++\")\n",
    "\n",
    "            # 検証用データの特徴量をロード\n",
    "            with open(test_s_pkl_file, 'rb') as DFfile:\n",
    "                testFeatureDf_s = pickle.load(DFfile)\n",
    "        else:\n",
    "            # 選択後の検証用データのpklなし\n",
    "            print(\"++ \" + test_s_pkl_file + \" なし ++\")\n",
    "\n",
    "            # 検証用のカラムを、訓練用に存在するやつだけ残す。無ければ列をデフォルト値で追加する。\n",
    "            # カラム順の並び替えも行う。\n",
    "            testFeatureDf_s = adjustTestDfColumn(trainFeatureDf_s, testFeatureDf)\n",
    "\n",
    "            # 検証用データの特徴量をダンプ(HyperOptExe用にダンプする)\n",
    "            with open(test_s_pkl_file, 'wb') as DFfile:\n",
    "                pickle.dump(testFeatureDf_s, DFfile, protocol=2)\n",
    "\n",
    "        #####################\n",
    "\n",
    "        # feature_特徴量上限数_事前評価方法 ディレクトリ\n",
    "        feature_dir = \"feature_\" + str(feature_num_lim_val) + \"_\" + evalDict.get(conf_eval_method) + \"/\"\n",
    "\n",
    "        # モデル選択\n",
    "        retClfPipe = getBestModelClf(trainFeatureDf_s, y_train, feature_num_lim_val, feature_dir, cv_num)\n",
    "\n",
    "        # ●メイン処理-予測 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "        # 予測\n",
    "        # 引数 学習済みモデル、検証用データの特徴量、予測種別、 クラスINDEX\n",
    "        resultDf = getPredictResultDf(retClfPipe, testFeatureDf_s, PREDICTTYPE_LABEL, TARGET_PROB_CLASS_VALUE)  #予測確率(クラス1)\n",
    "\n",
    "        # 不均衡調整モードを有効にしない限り必要の無い処理\n",
    "        ##idDf = idDf.reset_index(drop=True)\n",
    "\n",
    "        # IDと予測結果をjoin\n",
    "        resultDf = idDf.join(resultDf)\n",
    "\n",
    "        # ●メイン処理-ファイル出力 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "\n",
    "        # 選択されたモデルをファイルに残す。\n",
    "        if(type(retClfPipe) is GridSearchCV):\n",
    "            selectedModel = str(retClfPipe)\n",
    "        else:\n",
    "            selectedModel = str(retClfPipe.named_steps['est'])\n",
    "        print(selectedModel)\n",
    "        modelFilePath =  conf_output_folder_path + feature_dir + 'selected_model.txt'\n",
    "        with open(modelFilePath, mode='w') as f:\n",
    "            f.write(selectedModel)\n",
    "\n",
    "        #################\n",
    "\n",
    "        # 予測結果CSVファイル出力\n",
    "        fileOutput(resultDf, \"\", conf_submission_file, PRINT_DF_LINENUM, True, feature_dir)\n",
    "\n",
    "        # 実施リストに追加\n",
    "        feature_num_list.append(feature_num_lim_val)\n",
    "\n",
    "        feature_num_lim_val = getInputNumber(\"特徴量数(半角整数)\", \n",
    "                       \"\\r\\nこれで終了する場合は空文字のままEnterを押してください。 \\\n",
    "                        \\r\\nこれまでに予測した特徴量：\" + str(feature_num_list) + \"\")\n",
    "\n",
    "        if(feature_num_lim_val == None):\n",
    "            # ループ脱出\n",
    "            continueFlag = False\n",
    "    ############################################################################\n",
    "    ############################# ↑メインループ↑ #############################\n",
    "    ############################################################################\n",
    "\n",
    "    #終了\n",
    "    print(\"++++++++++++++++++++++++ END ++++++++++++++++++++++++\")\n",
    "\n",
    "\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "# メイン処理実行\n",
    "#####################################################################\n",
    "#####################################################################\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
